var suggestions=document.getElementById("suggestions"),search=document.getElementById("search");search!==null&&document.addEventListener("keydown",inputFocus);function inputFocus(e){e.ctrlKey&&e.key==="/"&&(e.preventDefault(),search.focus()),e.key==="Escape"&&(search.blur(),suggestions.classList.add("d-none"))}document.addEventListener("click",function(e){var t=suggestions.contains(e.target);t||suggestions.classList.add("d-none")}),document.addEventListener("keydown",suggestionFocus);function suggestionFocus(n){const s=suggestions.classList.contains("d-none");if(s)return;const e=[...suggestions.querySelectorAll("a")];if(e.length===0)return;const t=e.indexOf(document.activeElement);if(n.key==="ArrowUp"){n.preventDefault();const s=t>0?t-1:0;e[s].focus()}else if(n.key==="ArrowDown"){n.preventDefault();const s=t+1<e.length?t+1:t;e[s].focus()}}(function(){var e=new FlexSearch.Document({tokenize:"forward",cache:100,document:{id:"id",store:["href","title","description"],index:["title","description","content"]}});e.add({id:0,href:"/docs/dev/securite-applicative/",title:"Sécurité applicative",description:"Support\nTps",content:"Support\nTps\n"}).add({id:1,href:"/docs/ensai/projet-ensai/projet-ensai/",title:"Projet statistique",description:"Diaporama",content:"Diaporama\n"}).add({id:2,href:"/docs/ensai/seub/td/",title:"TD1",description:"Tableur du travail fait en séance",content:"Tableur du travail fait en séance\n"}).add({id:3,href:"/docs/ensai/classification/td1/",title:"TD1 Classification",description:"Méthodes géométriques",content:"Exercice 1 : K Means # 1-1 Différents paramétrages # | 1 | 2 | 9 | 12 | 20 |\n a) $K=2;μ_1=1;μ_2=7$\nK nombre de classe\nDans K-means, le nombre de classe est un paramètre, il est défini a priori\nOn choisit alors au hasard autant de points que de classes (notés $µ_i$), ce sont les barycentres des classes à l\u0026rsquo;itération 0.\nItération 1 #    1 2  9 12 20     x  x      $µ_1=1$  $µ_2=7$         Points plus proche de $µ_1$ que de $µ_2$ : 1,2\n  Points plus proche de $µ_2$ que de $µ_1$ : 9,12,20\n  Classes à l\u0026rsquo;itération 1 :\n| 1 2 || 9 12 20 |\nBarycentre de ces classes :\n Classe 1 : $(1+2)/2=1.5$ Classe 2 : $(9+12+20)/3=13.7$  Itération 2 #    1  2 9 12  20      x    x     $µ_1=1.5$    $µ_2=13.7$       Points plus proche de $µ_1$ que de $µ_2$ : 1,2\n  Points plus proche de $µ_2$ que de $µ_1$ : 9,12,20\n  Les classes sont stables :\n| 1 2 || 9 12 20 |\nFin de l\u0026rsquo;algorithme\n b) $K=2;μ_1=1;μ_2=20$\nItération 1 #    1 2 9 12 20       x    x     $µ_1=1$    $µ_2=20$        Points plus proche de $µ_1$ que de $µ_2$ : 1,2,9\n  Points plus proche de $µ_2$ que de $µ_1$ : 12,30\n  Classes à l\u0026rsquo;itération 1 :\n| 1 2 9 || 12 20 |\nBarycentre de ces classes :\n Classe 1 : $(1+2+9)/3=4$ Classe 2 : $(12+20)/2=16$  Itération 2 #    1 2  9 12  20       x   x      $µ_1=4$   $µ_2=16$       Points plus proche de $µ_1$ que de $µ_2$ : 1,2,9\n  Points plus proche de $µ_2$ que de $µ_1$ : 12,30\n  Les classes sont stables :\n| 1 2 9 || 12 20 |\nFin de l\u0026rsquo;algorithme\n Remarque : pour 2 tirages, on a obtenu deux classifications différentes\n=\u0026gt; Existance d\u0026rsquo;optimum locaux\n c)$K= 3,μ1= 1,μ2= 12 ,μ3= 20$\nItération 1 #    1 2 9 12 20     x   x x   $µ_1=1$   $µ_2=12$ $µ_3=20$     Points plus proche de $µ_1$ : 1,2 Points plus proche de $µ_2$ : 9,12 Points plus proche de $µ_3$ : 20  Classes à l\u0026rsquo;itération 1 :\n| 1 2 || 9 12 || 20 |\nBarycentre de ces classes :\n Classe 1 : $1.5$ Classe 2 : $10.5$ Classe 2 : $20$  Classes stables\n d)$K= 4,μ1= 1,μ2= 9,μ3= 12 ,μ4= 20$\nItération 1 #    1 2 9 12 20     x  x x x   $µ_1=1$  $µ_2=9$ $µ_3=12$ $µ_4=20$     Points plus proche de $µ_1$ : 1,2 Points plus proche de $µ_2$ : 9 Points plus proche de $µ_3$ : 12 Points plus proche de $µ_4$ : 20  Classes stables\n1-2 Un critère de sélection # On veut :\n Classes homogènes (= Inertie intra faible) Classes différentes entre elles (= Inertie inter forte)  Inertie totale = Inertie intra + Inertie inter\nPour une classe $k$, $I_{INTRA_k}=\\sum_{i\\in k}p_id²(x_i,G_k)$\nAvec :\n $i\\in k$ les éléments i de la classe k $G_k$ le barycentre de la classe k $d$ une distance à définir (paramètre de la méthode) $p_i$ les poids relatifs au sein de la classe ($\\sum_ip_i=1$ en sommant sur les individus de la classe). En l\u0026rsquo;absence d\u0026rsquo;indications, les individus sont équipondérés.  Au total, on pondère par les effectifs des classes :\n$I_{INTRA} = \\sum_{k=1}^K\\frac{n_k}{n}I_{INTRA_k}$\nSoit pour les classifications précédentes :\na)\nClasse {1 2} : Barycentre=1.5 $I_{INTRA}=1/2(0.5²+0.5²)=0.25$\nClasse {9 12 20} : Barycentre=13.7 $I_{INTRA}=1/3(4.7²+1.7²+6.3²)=21.6$\nSoit au total $I_{INTRA}=2/50.25+3/521.6=13.06$\nb)\nClasse {1 2 9} : Barycentre=4 $I_{INTRA}=1/3(3²+2²+5²)=12.67$\nClasse {12 20} : Barycentre=16 $I_{INTRA}=1/2(4²+4²)=16$\nSoit au total $I_{INTRA}=3/512.67+2/516=14$\nDans la classification obtenue avec b), les classes sont moins homogènes\nc)\nClasse {1 2} : Barycentre=0.5 $I_{INTRA}=1/2(0.5²+0.5²)=0.25$\nClasse {9 12} : Barycentre=10.5 $I_{INTRA}=1/2(1.5²+1.5²)=2.25$\nClasse {20} : Barycentre=20 $I_{INTRA}=0$\nSoit au total $I_{INTRA}=2/50.25+2/52.25+1/5*0=1$\nd)\n$I_{INTRA}=0.1$\n⚠️ Ce critère n\u0026rsquo;a de sens que si on compare des classifications avec le même nombre de classe !!! Avec plus de classe, on a FORCÉMENT moins d\u0026rsquo;inertie intra ! Sinon la meilleure classification est celle avec n classes singleton ($I_{INTRA}$ nulle !)\nExercice 2 : CAH # 2-1 Une première CAH à la main # Regrouper les individus en utilisant la classification hiérarchique ascendante et en prenant comme mesure de similarité la distance euclidienne et comme critère d’agrégation le linkage single.\n3 paramètres\n la méthode CAH, qui implique deux paramètres la mesure de similarité ~ distance entre points le critère d\u0026rsquo;aggrégation ~ distance entre classes  Rappel : distance euclidienne entre x et y : $\\sqrt{\\sum_{j=1}^p(x_j-y_j)²}$\nlinkage single ? $d(Classe_1,Classe_2)=min_{x\\in Classe_1,y\\in Classe_2}d(x,y)$ En d\u0026rsquo;autres termes, le plus petite distance possible entre un point de chacune des classe.\nalgorithme de CAH ? classification : ok hierarchique : l\u0026rsquo;output de la méthode sera une hiérachie dans laquelle on va choisir une partition (ou classification) ascendante : l\u0026rsquo;état initial de l\u0026rsquo;algorithme sera les singletons (5 classes de 1 employé)\nItération 1 # Classes : |E1|E2|E3|E4|E5|\nCalcul des distances entre individus :\n   Employé E1 E2 E3 E4 E5     Ancienneté 2 3 5 6 8   Salaire 2000 2100 3500 4100 10000        E1 E2 E3 E4 E5     E1 0 100 1500 2100 8000   E2  0 1400 2000 7900   E3   0 600 6500   E4    0 5900   E5     0    (arrondies au centième\nEntre E1 et E2 par exemple : $d(E1,E2)=\\sqrt{(2000-2100)²+(3-2)²}=\\sqrt{100²+1²}=\\sqrt{10001}\\approx \\sqrt{10000}$\nRemarque : On retrouve quasiment la distance en ne considérant que la variable Salaire\nLes deux individus (classes singletons) les plus proches sont E1 et E2, on les regoupe\nItération 2 # Classes : |E1 E2|E3|E4|E5|\nDisatances entre classe selon le critère du linkage single (distance minimale)\n    E1 E2 E3 E4 E5      E1 E2 0 1400 2000 7900    E3  0 600 6500    E4   0 5900    E5    0     Les deux classes les plus proches : singletons E3 et E4\nItération 3 #     E1 E2 E3 E4 E5       E1 E2 0 1400 2000 7900    E3 E4  0 5900     E5   0      Les deux classes les plus proches : {E1 E2} et {E3 E4}\nItération 4 #     E1 E2 E3 E4 E5        E1 E2 E3 E4 0 5900      E5  0       Forcément on regroupe les deux classes restantes et on obtient une classe avec tout le monde\nChoisir une classification\nLa méthode ne donne pas une mais une succesion de classification plausible\nIl faut savoir quand s\u0026rsquo;arrêter\nPas facile avec la méthode linkage simple, souvent on utilise le critère de Ward. La distance entre classe est alors le regroupe ment qui fait perdre le moins d\u0026rsquo;inertie intra.\nAvec le critère de Ward, on peut prendre en compte la \u0026ldquo;vitesse\u0026rdquo; de gain d\u0026rsquo;inertie intra lors d\u0026rsquo;un regroupement\nRemarque : sous R, en utilisant FactoMineR (qui utilise lui même agnes), on peut choisir comme méthode :\n method: character string defining the clustering method. The six methods implemented are '\u0026quot;average\u0026quot;' ([unweighted pair-]group [arithMetic] average method, aka 'UPGMA'), '\u0026quot;single\u0026quot;' (single linkage), '\u0026quot;complete\u0026quot;' (complete linkage), '\u0026quot;ward\u0026quot;' (Ward's method), '\u0026quot;weighted\u0026quot;' (weighted average linkage, aka 'WPGMA'), its generalization '\u0026quot;flexible\u0026quot;' which uses (a constant version of) the Lance-Williams formula and the 'par.method' argument, and '\u0026quot;gaverage\u0026quot;' a generalized '\u0026quot;average\u0026quot;' aka \u0026quot;flexible UPGMA\u0026quot; method also using the Lance-Williams formula and 'par.method'.  2-2 2-3 Valeurs standardisées # Standardiser (réduire) ? un souvenir des différences entre ACP normées ou non ?\nanciennete salaire [1,] -1.17279094 -0.71128234 [2,] -0.75393703 -0.68088566 [3,] 0.08377078 -0.25533212 [4,] 0.50262469 -0.07295204 [5,] 1.34033251 1.72045216  Disparition des problèmes d\u0026rsquo;ordre de grandeur entre les variables ancienneté et salaire. Même problématique que l\u0026rsquo;ACP normée : résoudre les problèmes d\u0026rsquo;unité, de dimension,\u0026hellip;\n nouvelles distances  [,1] [,2] [,3] [,4] [,5] [1,] 0 0.42 1.34 1.79 3.50 [2,] 0 0.00 0.94 1.40 3.19 [3,] 0 0.00 0.00 0.46 2.34 [4,] 0 0.00 0.00 0.00 1.98 [5,] 0 0.00 0.00 0.00 0.00   Code\nrm(list=ls()) anciennete=c(2,3,5,6,8) salaire=c(2000,2100,3500,4100,10000) data=cbind(anciennete,salaire) data=as.data.frame(data) # On réduit les données dataScale=as.data.frame(scale(data)) # Calcul des distances ## A la main distance= function(i,j,data) { sqrt( rowSums((data[i,]-data[j,])**2 ) ) } distances= function(data){ a= matrix(data = rep(0, 25), nrow = 5, ncol = 5) for (i in 1:5) { for (j in i:5) { a[i,j]=distance(i,j,data) } } round(a,2) } distances(data) distances(dataScale) ## Avec la fonction daisy round(daisy(data),2) round(daisy(dataScale),2) # CAH require(cluster) cah \u0026lt;- agnes(data, metric = \u0026quot;euclidean\u0026quot;, method=\u0026quot;single\u0026quot;,stand = FALSE) plot(cah) require(FactoMineR) res.HCPC=HCPC(data,method=\u0026quot;single\u0026quot;) res.HCPC=HCPC(dataScale,method=\u0026quot;single\u0026quot;)   Compléments\n ACP + CAH : on effectue une ACP, on utilise un critère de sélection d\u0026rsquo;axe, et on effectue la CAH sur les données nettoyées\u0026quot;  # Pour ne conserver que les 3 premiers axes res.PCA\u0026lt;-PCA(data,scale.unit=TRUE,graph=FALSE,ncp=3) # CAH qui utilise uniquement les 3 premiers axes factoriels res.HCPC=HCPC(res.PCA)   Variables qualitatives ? : on effectue une ACM (on sélectionne les axes) et la CAH se fait sur les axes factoriels  # Pour ne conserver que les 3 premiers axes res.MCA\u0026lt;-MCA(data,scale.unit=TRUE,graph=FALSE,ncp=3) # CAH qui utilise uniquement les 3 premiers axes factoriels de l'ACM res.HCPC=HCPC(res.MCA)  Exercice 3 (Kmeans et variables qualitatives) # On considère une enquête conduite par deux étudiants d\u0026rsquo;Agrocampus sur 135 personnes visant à avoir une vue d\u0026rsquo;ensemble de leurs différentes prises de position concernant les OGM (fichier ogm.csv). Il leur a été posé un ensemble de 21 questions fermées que nous avons réparties en deux groupes. Un premier groupe composé de seize questions en lien direct avec le rapport aux OGM qu\u0026rsquo;ont les per- sonnes interrogées :\n Vous sentez-vous concerné par la polémique sur les OGM. Quelle est votre position quand à la culture d\u0026rsquo;OGM en France. Quelle est votre position quant à l\u0026rsquo;incorporation de matrière première OGM dans les produits alimentaires destinés à l\u0026rsquo;alimentation humaine. Quelle est votre position quant à l\u0026rsquo;incorporation de matrière première OGM dans les produits alimentaires destinés à l\u0026rsquo;alimentation animale. Avez-vous déjà participé à une manifestation contre les OGM. Estimez-vous que les médias communiquent suffisamment sur le sujet. Faîtes-vous vous-même la démarche de vous informer sur le sujet. Pensez-vous que les OGM puissent permettre la réduction d\u0026rsquo;usage de fongicides. Pensez-vous que les OGM puissent permettre la réduction des problèmes de famine dans le monde. Pensez-vous que les OGM puissent permettre l\u0026rsquo;amélioration des conditions de vie des agriculteurs. Pensez-vous que les OGM puissent permettre de futurs progrès scientifiques. Pensez-vous que les OGM représentent un éventuel danger pour notre santé. Pensez-vous que les OGM représentent une menace pour l\u0026rsquo;environnement. Pensez-vous que les OGM représentent un risque économique pour les agriculteurs. Pensez-vous que les OGM représentent un procédé scientifique inutile. Pensez vous que nos grands-parents avaient une alimentation plus saine.  Un second groupe composé de cinq variables de signalétique au sens large :\n Sexe Catégorie soci-professionnelle Tranche d\u0026rsquo;âge Exercez vous des études ou un métier en rapport avec l\u0026rsquo;agriculture, l\u0026rsquo;agroalimentaire ou la pharmaceutique A quel parti politique vous identifiez vous le plus  Problématique. À travers ce questionnaire, on cherche à obtenir une typologie des personnes interrogées en fonction de leur rapport aux OGM; à voir si cette typologie n\u0026rsquo;est pas sans lien avec les variables de signalétique d\u0026rsquo;autre part.\n Quels sont les éléments actifs et les élements supplémentaires? Quelle analyse factorielle faut-il effectuer pour étudier les dépendances entre les variables? On souhaite effectuer une classiffication non supervisée afin de déterminer des groupes d\u0026rsquo;opinion relatifs aux OGM. Pour cela, on souhaite utiliser la méthode des K-means. À partir des sorties logicielles, répondre aux questions suivantes  Quelle distance a été utilisée ? Combien de classes faudrait-il considérer ? Cela est-il pertinent avec les sorties de la méthode factorielle ? Interpréter les différentes classes. Il y a-t-il un lien significatif entre la position vis à vis des OGM et le genre ? Il y a-t-il un lien significatif entre la position vis à vis des OGM et la sensibilité politique ?    rm(list=ls()) require(FactoMineR) ogm \u0026lt;- read.table(\u0026quot;./Classification/data/ogm.csv\u0026quot;,header = TRUE,sep=\u0026quot;;\u0026quot;) dim(ogm) # Traitement des modalités rares (regroupements de Très favorable et Favorable) plot(ogm$Position.Al.H) prop.table(table(ogm$Position.Al.H)) levels(ogm$Position.Al.H)[4] \u0026lt;- levels(ogm$Position.Al.H)[1] plot(ogm$Position.Culture) prop.table(table(ogm$Position.Culture)) levels(ogm$Position.Culture) \u0026lt;- c(\u0026quot;Favorable\u0026quot;, \u0026quot;Pas Favorable du Tout\u0026quot;, \u0026quot;Plutot Defavorable\u0026quot;, \u0026quot;Favorable\u0026quot;) # On ne fait la classification que sur les variables sur les avis sur les OGM dataActif \u0026lt;- ogm[,1:16] dataToCluster \u0026lt;- tab.disjonctif(dataActif) # Comparaison de k-means sur plusieurs nombre de classes Kmax \u0026lt;- 10 results \u0026lt;- list() criterion \u0026lt;- rep(NA, Kmax) for (k in 1:Kmax){ results[[k]] \u0026lt;- kmeans(dataToCluster, k) criterion[k] \u0026lt;- results[[k]]$tot.withinss } plot(1:Kmax, criterion, xlab=\u0026quot;Number of clusters\u0026quot;, ylab=\u0026quot;Criterion\u0026quot;) # Sélection de 3 classes avec \u0026quot;critère du coude\u0026quot; Kselec \u0026lt;- 3 by(ogm[,1:16], results[[Kselec]]$cluster, summary) # Analyse fine d'une variable \u0026quot;active\u0026quot; = utilisée dans la classification contingence = table(ogm$Position.Al.H,results[[Kselec]]$cluster) contingence = addmargins(contingence,2) prop.table(contingence,2) # Analyse fine d'une variable \u0026quot;supplémentaire\u0026quot; = non utilisée dans la classification contingence = table(ogm$Parti.Politique,results[[Kselec]]$cluster) contingence = addmargins(contingence,2) prop.table(contingence,2) # Comparaison avec ACM res \u0026lt;- MCA(ogm, ncp = 5, quali.sup = 17:21, graph = FALSE) plot(res$ind$coord[,1], res$ind$coord[,2], col=results[[Kselec]]$cluster, pch=results[[Kselec]]$cluster) by(ogm[,17:21], results[[Kselec]]$cluster, summary) # Comparaison lien Sexe/OGM et Politique/OGM chisq.test(results[[Kselec]]$cluster, ogm$Sexe) contingence = table(ogm$Sexe,results[[Kselec]]$cluster) contingence = addmargins(contingence,2) contingence prop.table(contingence,2) chisq.test(results[[Kselec]]$cluster, ogm$Parti.Politique) contingence = table(ogm$Parti.Politique,results[[Kselec]]$cluster) contingence = addmargins(contingence) contingence prop.table(contingence,2)  "}).add({id:4,href:"/docs/ensai/classification/td2/",title:"TD2 Classification",description:"Méthodes probabilistes",content:"Exercice 1 : Mélanges et algorithme EM # Préambule #  Qu\u0026rsquo;est ce qu\u0026rsquo;un \u0026ldquo;mélange\u0026rdquo; de lois ?  On considère un échantillon $x= (x1,\u0026hellip;,xn)$ composé de $n$ observations indépendantes $x_i∈X$. Chaque observation est issue d’un mélange à $K$ composantes déterminées par $p(x_i;\\theta)=\\sum_{k=1}^{K}\\pi_kp_k(x_i;\\alpha_k)$,avec $\\theta \\in \\Theta$.\n = loi inconnue compliquée à décrire, à paramétrer  = 2 lois normales superposées Idée :\n chaque point a une probabilité $\\pi_k$ d\u0026rsquo;appartenir à une des deux courbes (= proportion des points correspondant à chacune des 2 courbes, bien entendue inconnue), $\\sum \\pi_k =1$ la loi de chaque courbe est $p_k(x_i;\\alpha_k)$ avec $\\alpha_k$ les paramètres de la loi $p_k$ (si $p_k$ est une loi normale $\\alpha_k=(\\mu_k,\\sigma_k)$) Ce qui donne en additionnant chaque loi probable $p(x_i;\\theta)=\\sum_{k=1}^{K}\\pi_kp_k(x_i;\\alpha_k)$  a)\n log-vraisemblance $l(\\theta;x)$  Vraissemblance : $L(\\theta,x)=\\prod_{i=1}^np(x_i;\\theta)$\n$l(\\theta;x)$ = $\\sum_{i=1}^{n}ln(p(x_i;\\theta))$ = $\\sum_{i=1}^{n}ln(\\sum_{k=1}^{K}\\pi_kp_k(x_i;\\alpha_k))$\nOn obtient une équation sous une forme difficilement estimable (même par optimisation par un logiciel)\n log-vraisemblance complétée $l(\\theta;x,z)$  Idée : si on sait à quelle classe appartient chaque point, la loi est beaucoup plus simple\nOn introduit la variable catégorielle de l\u0026rsquo;appartenance à la classe : $Classe_i=k$ Pour rester numérique on choisit de la noter avec des variables booléenne $Z_k$, telle que $z_{i,k}=1$ si $Classe_i=k$, sinon $z_{i,k}=0$\nPour le point $i$, on ne retient plus que la composante auquel il appartient\nLa \u0026ldquo;contribution\u0026rdquo; à la log-vraissembalnce du point $i$ appartenant à la classe k_0 est donc : $$ ln(\\pi_{k_0}p_{k_0}(x_i;\\alpha_{k_0}))=z_{i_{k_0}}ln(\\pi_{k_0}p_{k_0}(x_i;\\alpha_{k_0})) $$ car $z_{i_{k_0}}=1$ $$ =\\sum_{k=1}^{K}z_{i,k}ln(\\pi_kp_k(x_i;\\alpha_k)) $$ car $z_{i_{k}}=0, \\forall k\\neq k_0$\nsoit\n$l(\\theta;x,z)=\\sum_{i=1}^{n}\\sum_{k=1}^{K}z_{i,k}ln(\\pi_kp_k(x_i;\\alpha_k))$\nle modèle reste complexe, mais peut être résolu par l\u0026rsquo;algorithme EM qui va avoir le bon gout de nous donner une estimation des $Z_k$ (et donc une classification)\nb)\n$t_{i,k}(\\theta)$ probabilité a posteriori sous le paramètre $\\theta$ que l’observation $i$ soit issue de la composante $k$. ⚠️ les $\\pi_k$ sont les probabilité a priori et font partie de $\\theta$ !!!\nCalcul de probabilité à posteriori (Bayes) $$ t_{i,k}(\\theta)=P_\\theta(z_{i,k}=1|X_i=x_i)=\\frac{P_\\theta(z_{i,k}=1)P_\\theta(X_i=x_i|z_{i,k}=1)}{P_\\theta(X_i=x_i)} =\\frac{\\pi_kp_k(x_i;\\alpha_k)}{p(x_i;\\theta)} $$\n=\u0026gt; $t_{i,k}$ est donc l\u0026rsquo;espérance conditionnelle de $z_{i,k}$ sachant $x_i$ (espérance d\u0026rsquo;un Bernoulli B = P(B=1))\n=\u0026gt; pour un i donné, la meilleure espérance parmi les k nous donne la classe la plus probable. Donc non seulement on va obtenir une classification, mais en plus on sera capable de mesurer à quel point un point est vraiment sûr dans une classe ou en ballotage (cas avec des probas 0.9/0.1 VS probas 0.49/0.51)\nc)\nSoit $E(t(\\theta),z)=-\\sum_{i=1}^{n}\\sum_{k=1}^Kz_{i,k}ln(t_{i,k}(\\theta))$, avec $t(\\theta) = (t_{i,k}(\\theta);i= 1,\u0026hellip;,n;k= 1,\u0026hellip;,K)$ (vecteur des probabilités a posteriori)\nVérifier la formule d’Hathaway : $l(\\theta;x) =l(\\theta;x,z) +E(t(\\theta),z)$\nÀ partir du calcul de $t_{i_k}$ précédent, que l\u0026rsquo;on passe au logarithme : $ln(t_{i,k}(\\theta))=ln(\\pi_kp_k(x_i;\\alpha_k))-ln(p(x_i;\\theta))$\nOn écrit l\u0026rsquo;équation $K$ fois en multipliant par $z_{i,k}$ chaque ligne, et on somme les lignes\n$$ \\implies \\sum_{k=1}^Kz_{ik}ln(t_{i,k}(\\theta)) =\\sum_{k=1}^Kz_{ik}ln(\\pi_kp_k(x_i;\\alpha_k))-ln(p(x_i;\\theta))\\sum_{k=1}^Kz_{i_k} $$ ($ln(p(x_i;\\theta))$ ne dépend pas de $k$)\n$\\iff ln(p(x_i;\\theta)) = \\sum_{k=1}^Kz_{i,k}ln(\\pi_kp_k(x_i;\\alpha_k)) - \\sum_{k=1}^Kz_{i,k}ln(t_{i,k}(\\theta))$\nSoit en sommant sur i : $l(\\theta;x) = l(\\theta,x,z) + E(t(\\theta),z)$\nL\u0026rsquo;algorithme EM # EM = espérance - maximisation\nParamétrage \u0026ldquo;fixe\u0026rdquo; : le nombre de classe K\nInitialisation : choix d\u0026rsquo;un paramètre $\\theta^{[0]}$ : $\\theta$ est constitué des $\\pi_k$ + des vecteurs $\\alpha_k$ de chaque loi constituant le mélange Exemple : mélange de 3 lois normales : 9 paramètres, les 3 proportions de classes + la moyenne et la variance de chaque classe (en réalité 8 paramètres car la dernière proportion est contrainte par 100-les deux premières)\nSur une itération $r$\n  l\u0026rsquo;étape d\u0026rsquo;espérance : calcul de la logvraissamblance complété passant par le calcul des $t_{i_k}(\\theta^{[r-1]})$ En effet la log vraissemblance complété vaut $l(\\theta;x,z)=\\sum_{i=1}^{n}\\sum_{k=1}^{K}z_{i,k}ln(\\pi_kp_k(x_i;\\alpha_k))$, comme on ne connait pas Z, on remplace par son espérance qui est donc $t_{i_k}$\n  l\u0026rsquo;étape de maximisation : on maximise l\u0026rsquo;espérance de la vraissamblance complétée, c\u0026rsquo;est à dire recherche du nouveau $\\theta^{[r]}$ qui maximise $\\sum_i\\sum_kt_{i,k}(\\theta^{[r-1]})ln(\\pi_kp_k(x_i;\\alpha_k))$\n  On itère jusqu\u0026rsquo;à stabilisation de la vraissemblance\n⚠️ initialisation aléatoire =\u0026gt; possibilité de tomber sur un maximum local =\u0026gt; lancer l\u0026rsquo;algorithme pusieurs fois et conserver le meilleur résultat\nConvergence de l\u0026rsquo;algorithme # $\\Theta$ est l\u0026rsquo;espace des paramètres possibles\nSoit $\\theta^{[r]} \\in \\Theta,\\theta^{[r-1]} \\in \\Theta$, montrer que $$ l(\\theta^{[r]};x)=Q(\\theta^{[r]};\\theta^{[r-1]}) - H( \\theta^{[r]};\\theta^{[r-1]}) $$ avec $$ Q(\\theta^{[r]};\\theta^{[r-1]}) =E_{\\theta^{[r-1]}}[l(\\theta^{[r]};x,z)|X_i=x_i] $$ $$ H(\\theta^{[r]};\\theta^{[r-1]}) =\\sum_{i=1}^n\\sum_{k=1}^KE_{\\theta^{[r-1]}}[z_{i,k}ln(t_{ik}(\\theta^{[r]}))|X_i=x_i] $$ et $E_{\\theta^{[r-1]}}[.]$ l\u0026rsquo;espérance prise en considérant la distribution mélange avec le paramère $\\theta^{[r-1]}$.\nRappel : à chaque étape on cherche à maximiser un paramètre $\\theta^{[r]}$ en se servant de la valeur qu\u0026rsquo;il avait à l\u0026rsquo;itération prédédente pour calculer la vraissamblance complétée. dans les écritures de $H$ et $Q$, $\\theta^{[r-1]}$ est le paramètre de l\u0026rsquo;itération précédente, $\\theta^{[r]}$ est le nouveau paramètre à rechercher pour maximiser la vraissemblance.\n$$ l(\\theta^{[r]};x)=\\sum_{i=1}^n ln(p(xi;\\theta^{[r]}))= \\sum_{i=1}^n\\sum_{k=1}^Kz_{i,k}ln(\\pi_k^{[r]}p_k(x_i;\\alpha_k^{[r]})) - \\sum_{i=1}^n\\sum_{k=1}^Kz_{i,k}ln(t_{i,k}(\\theta^{[r]})) $$ (Voir formule d\u0026rsquo;Hathaway plus haut)\nOn passe à l\u0026rsquo;espérance sous le paramètre $\\theta^{[r-1]}$ conditionnellement à $X_i=x_i$ La seule chose aléatoire, ce sont les $z_{i,k}$, qui vont dont être estimés par leur espérance qui se calcule dans le cadre d\u0026rsquo;un paramétrage, ici celui de l\u0026rsquo;itération précédente $\\theta^{[r-1]}$ $$ \\sum_{i=1}^n E_{\\theta^{[r-1]}}[ln(p(xi;\\theta^{[r]}))|X_i=x_i]= \\sum_{i=1}^n\\sum_{k=1}^KE_{\\theta^{[r-1]}}[z_{i,k}ln(\\pi_k^{[r]}p_k(x_i;\\alpha_k^{[r]}))|X_i=x_i] - \\sum_{i=1}^n\\sum_{k=1}^KE_{\\theta^{[r-1]}}[z_{i,k}ln(t_{i,k}(\\theta^{[r]}))|X_i=x_i] $$ le premier terme est déterministe, il s\u0026rsquo;agit de l\u0026rsquo;espérance d\u0026rsquo;un scalaire $$ \\iff \\sum_{i=1}^n ln(p(xi;\\theta^{[r]})) = \\sum_{i=1}^n\\sum_{k=1}^KE_{\\theta^{[r-1]}}[z_{i,k}ln(\\pi_k^{[r]}p_k(x_i;\\alpha_k^{[r]}))|X_i=x_i] - \\sum_{i=1}^n\\sum_{k=1}^KE_{\\theta^{[r-1]}}[z_{i,k}ln(t_{i,k}(\\theta^{[r]}))|X_i=x_i] $$ on resimplifie ensuite les expressions de log vraissemblance et log vraissemblance complétée pour revenir à la formule $$ \\iff l(\\theta^{[r]};x) = E_{\\theta^{[r-1]}}[l(\\theta^{[r]};x,z)|X_i=x_i] - \\sum_{i=1}^n\\sum_{k=1}^KE_{\\theta^{[r-1]}}[z_{i,k}ln(t_{i,k}(\\theta^{[r]}))|X_i=x_i] $$ $$ \\iff l(\\theta^{[r]};x) = Q(\\theta^{[r]},\\theta^{[r-1]}) - H(\\theta^{[r]},\\theta^{[r-1]}) $$\nÉcrire la différence entre les log-vraisemblances obtenues aux itérations $r$ et $r-1$ à partir des fonctions Q et H\n$$ l(\\theta^{[r]};x)-l(\\theta^{[r-1]};x)=Q(\\theta^{[r]},\\theta^{[r-1]}) - Q(\\theta^{[r-1]},\\theta^{[r-1]}) - H(\\theta^{[r]},\\theta^{[r-1]}) + H(\\theta^{[r-1]},\\theta^{[r-1]}) $$\nOn s\u0026rsquo;intéresse donc à l\u0026rsquo;évolution de la vraissemblance entre l\u0026rsquo;ancienne valeur (itération précédente) $\\theta^{[r-1]}$ et la nouvelle valeur $\\theta^{[r]}$, pour cela on le décompose sous le paramétrage de l\u0026rsquo;itération précédente fixé $\\theta^{[r-1]}$\nMontrer que : $Q(\\theta^{[r]};\\theta^{[r-1]})- Q(\\theta^{[r-1]};\\theta^{[r-1]})\\geq0$\n$$ Q(\\theta^{[r]};\\theta^{[r-1]}) = \\max_{\\theta \\in \\Theta} Q(\\theta;\\theta^{[r-1]}) $$ en effet, par construction de l\u0026rsquo;algorithme EM $$ \\theta^{[r]} = \\arg\\max_{\\theta \\in \\Theta} \\sum_{i=1}^{n}\\sum_{k=1}^{K}t_{i,k}(\\theta^{[r-1]})ln(\\pi_kp_k(x_i;\\alpha_k)) = \\arg\\max_{\\theta \\in \\Theta} E_{\\theta^{[r-1]}}[l(\\theta;x,z)|X_i=x_i] $$ et $Q(\\theta;\\theta^{[r-1]}) =E_{\\theta^{[r-1]}}[l(\\theta;x,z)|X_i=x_i]$ La quantité $Q$ n\u0026rsquo;est rien d\u0026rsquo;autre que la vraissemblance complété calculée avec $\\theta^{[r-1]}$ $$ \\implies Q(\\theta^{[r]};\\theta^{[r-1]}) ≥ Q(\\theta^{[r-1]};\\theta^{[r-1]}) $$\nEn utilisant l’inégalité de Jensen, montrer que : $H(\\theta^{[r]};\\theta^{[r-1]})-H(\\theta^{[r-1]};\\theta^{[r-1]})≤0$\n$$ H(\\theta^{[r]};\\theta^{[r-1]})-H(\\theta^{[r-1]};\\theta^{[r-1]}) $$ $$ =\\sum_{i=1}^n\\sum_{k=1}^nE_{\\theta^{[r-1]}}[z_{i,k}ln(t_{ik}(\\theta^{[r]}))|X_i=x_i] - \\sum_{i=1}^n\\sum_{k=1}^nE_{\\theta^{[r-1]}}[z_{i,k}ln(t_{ik}(\\theta^{[r-1]}))|X_i=x_i] $$\nAttention : $ln(t_{ik}(\\theta^{quelconque}))$ est déterministe ! La seule chose aléatoire dans les espérance ce sont les $z_{i,k}$\n$$ =\\sum_{i=1}^n\\sum_{k=1}^n ln(t_{ik}(\\theta^{[r]})) E_{\\theta^{[r-1]}}[z_{i,k}|X_i=x_i] - \\sum_{i=1}^n\\sum_{k=1}^n ln(t_{ik}(\\theta^{[r-1]})) E_{\\theta^{[r-1]}}[z_{i,k}|X_i=x_i] $$ et $E_{\\theta^{[r-1]}}[z_{i,k}|X_i=x_i]$ est la définition de $t_{i,k}(\\theta^{[r-1]})$ $$ =\\sum_{i=1}^n\\sum_{k=1}^n t_{ik}(\\theta^{[r-1]})[ln(t_{ik}(\\theta^{[r]})) - ln(t_{ik}(\\theta^{[r-1]}))] $$ $$ =\\sum_{i=1}^n\\sum_{k=1}^n t_{ik}(\\theta^{[r-1]})ln(\\frac{t_{ik}(\\theta^{[r]})}{t_{ik}(\\theta^{[r-1]})}) $$\nOn utilise Jensen et la concavité du logarithme ainsi : $$ \\sum_{k=1}^Ku_kln(v_k) \\leq ln(\\sum_{k=1}^Ku_kv_k) $$ si les $u_k$ positifs et $\\sum u_k=1$\n$t_{ik}(\\theta^{[r-1]}) \\geq 0$ et $\\sum_k t_{ik}(\\theta^{[r-1]})=1$ car i appartient forcément à une des classes, donc la somme des probabilités d\u0026rsquo;appartenance à chacune des classes vaut 1\nOn remplace donc $u_k$ par $t_{ik}(\\theta^{[r-1]})$ et $v_k$ par $\\frac{t_{ik}(\\theta^{[r]})}{t_{ik}(\\theta^{[r-1]})}$ et on conclue\n$$ H(\\theta^{[r]};\\theta^{[r-1]})-H(\\theta^{[r-1]};\\theta^{[r-1]})≤\\sum_{i=1}^nln(\\sum_{k=1}^Kt_{ik}(\\theta^{[r]}))=0 $$\nEn cumulant les deux résultats précédents, on aboutit à $$ l(\\theta^{[r]};x)-l(\\theta^{[r-1]};x)\\geq 0 \\iff l(\\theta^{[r]};x) \\geq l(\\theta^{[r-1]};x) $$\nOn obtient qu\u0026rsquo;à chaque itération la vraissemblance augmente forcément, ce qui assure que l\u0026rsquo;algorithme va forcément converger vers un maximum local. (Au pire on stagne, ce qui sgnifie qu\u0026rsquo;on a atteint ce maximum)\nExercice 2 : Mélanges de distribution de Poisson # La loi de Poisson est une loi discrète définie par $p(X=x)=\\frac{\\lambda^x}{x!}e^{-\\lambda}$ Elle posède donc un paramètre : $\\lambda$\nOn considère un échantillon $x= (x_1,\u0026hellip;,x_n)$ composé de $n$ observations indépendantes $x_i∈\\mathbb{N}$. Pour effectuer le clustering, on suppose que les données sont issues d’un mélange de distributions de Poisson à $K$ composantes.\nSi on mélange $K$ lois de poisson, on a donc $2K$ paramètres à estimer, $K$ $\\lambda_k$ et $K$ $\\pi_k$ (donc un forcé par contrainte $\\sum \\pi_k =1$) $\\theta = (\\lambda_1,\u0026hellip;,\\lambda_K,\\pi_1,\u0026hellip;,\\pi_K)$\nÉcriture générale $$ p(x_i;\\theta) = \\sum_{k=1}^K \\pi_k p_k(x_i;\\lambda_k) $$ ici : $$ p(x_i;\\theta) = \\sum_{k=1}^K \\pi_k \\frac{{\\lambda_k}^{x_i}}{x_i!}e^{-\\lambda_k} $$ Les $\\lambda_k$ sont positifs, et différents 2 à 2 (si 2 sont identiques, c\u0026rsquo;est la même composante)\nDans ces conditions, le modèle est-il identifiable ? C\u0026rsquo;est à dire : si $\\forall x \\in \\mathbb{N}, p(x,\\theta)=p(x,\\tilde \\theta) \\implies \\theta = \\tilde \\theta$ Dans le cas d\u0026rsquo;un mélange l\u0026rsquo;unicité se définit \u0026ldquo;à permutation des composantes près\u0026rdquo; Pour cela on choisit par exemple de trier les $\\lambda_k$ du plus grand au plus petit : contrainte sur \u0026ldquo;l\u0026rsquo;ordre\u0026rdquo; des composantes\nSupposons $\\forall x \\in \\mathbb{N}, p(x,\\theta)=p(x,\\tilde \\theta)$ On choisit que si $\\lambda_1 \\neq \\tilde \\lambda_1$ alors on \u0026ldquo;classe\u0026rdquo; $\\theta$ et $\\tilde \\theta$ tel que $\\lambda_1 \u0026gt; \\tilde \\lambda_1$, c\u0026rsquo;est à dire qu\u0026rsquo;on suppose $\\lambda_1 \\geq \\tilde \\lambda_1$\n$$ p(x,\\theta)=p(x,\\tilde \\theta) $$ $$ \\iff \\sum_{k=1}^K \\pi_k \\frac{{\\lambda_k}^{x_i}}{x_i!}e^{-\\lambda_k}=\\sum_{k=1}^K \\tilde \\pi_k \\frac{{\\tilde \\lambda_k}^{x_i}}{x_i!}e^{-\\tilde \\lambda_k} $$ $$ \\iff \\pi_1 \\frac{{\\lambda_1}^{x_i}}{x_i!}e^{-\\lambda_1} + \\sum_{k=2}^K \\pi_k \\frac{{\\lambda_k}^{x_i}}{x_i!}e^{-\\lambda_k}=\\tilde \\pi_1 \\frac{{\\tilde \\lambda_1}^{x_i}} {x_i!}e^{-\\tilde \\lambda_1} + \\sum_{k=2}^K \\tilde \\pi_k \\frac{{\\tilde \\lambda_k}^{x_i}}{x_i!}e^{-\\tilde \\lambda_k} $$ $$ \\iff 1 + \\sum_{k=2}^K \\frac{\\pi_k}{\\pi_1} \\frac{{\\lambda_k}^{x_i}}{{\\lambda_1}^{x_i}}e^{\\lambda_1-\\lambda_k}=\\frac{\\tilde \\pi_1}{\\pi_1} \\frac{{\\tilde \\lambda_1}^{x_i}}{{\\lambda_1}^{x_i}}e^{\\lambda_1 - \\tilde \\lambda_1} + \\sum_{k=2}^K \\frac{\\tilde \\pi_k}{\\pi_1} \\frac{{\\tilde \\lambda_k}^{x_i}}{{\\lambda_1}^{x_i}}e^{\\lambda_1-\\tilde \\lambda_k} $$ Par hypothèse, on a $\\lambda_1\u0026gt;\\lambda_k$ et $\\lambda_1 \\geq \\tilde \\lambda_1 \u0026gt; \\tilde \\lambda_k$ Donc : $\\forall k\u0026gt;1,\\frac{\\lambda_k}{\\lambda_1}\u0026lt;1$ $\\forall k\u0026gt;1,\\frac{\\tilde \\lambda_k}{\\lambda_1}\u0026lt;1$ Donc : $\\forall k\u0026gt;1,\\lim_{x\\to\\infty}\\frac{\\pi_k}{\\pi_1}(\\frac{\\lambda_k}{\\lambda_1})^{x_i}e^{(\\lambda_1-\\lambda_k)}=0$ $\\forall k\u0026gt;1,\\lim_{x\\to\\infty}\\frac{\\pi_k}{\\pi_1}(\\frac{\\tilde \\lambda_k}{\\lambda_1})^{x_i}e^{(\\lambda_1-\\tilde \\lambda_k)}=0$\nSoit en revenant à l\u0026rsquo;égalité précédente $$ \\iff 1 = \\lim_{x\\to\\infty}\\frac{\\tilde \\pi_1}{\\pi_1} \\frac{{\\tilde \\lambda_1}^{x_i}}{{\\lambda_1}^{x_i}}e^{\\lambda_1 - \\tilde \\lambda_1} $$ et $$ \\lim_{x\\to\\infty} \\frac{\\tilde \\pi_1}{\\pi_1} \\frac{{\\tilde \\lambda_1}^{x_i}}{{\\lambda_1}^{x_i}}e^{\\lambda_1 - \\tilde \\lambda_1} = \\frac{\\tilde \\pi_1}{\\pi_1} \\text{ si } \\lambda_1 = \\tilde \\lambda_1 $$ $$ \\lim_{x\\to\\infty} \\frac{\\tilde \\pi_1}{\\pi_1} \\frac{{\\tilde \\lambda_1}^{x_i}}{{\\lambda_1}^{x_i}}e^{\\lambda_1 - \\tilde \\lambda_1} = 0\\text{ si } \\lambda_1 \u0026gt; \\tilde \\lambda_1 $$\nDonc (par l\u0026rsquo;absurde) $\\lambda_1 = \\tilde \\lambda_1$ et de fait $\\pi_1 = \\tilde \\pi_1$\nUne fois qu\u0026rsquo;on a montré le résultat pour ce premier paramètre on effectue le même raisonnement pour les paramètres suivant\nEstimation par maximum de vraissemblance # $$ l(\\theta;x)=\\sum_{i=1}^nln(\\sum_{k=1}^n \\pi_k \\frac{\\lambda_k^{x_i}}{x_i!}e^{-\\lambda_k}) $$ $$ l(\\theta;x,z)=\\sum_{i=1}^n\\sum_{k=1}^nz_{i,k}ln( \\pi_k \\frac{\\lambda_k^{x_i}}{x_i!}e^{-\\lambda_k}) $$ $$ =\\sum_{i=1}^n\\sum_{k=1}^nz_{i,k}(ln( \\pi_k ) + x_iln(\\lambda_k) - ln(x_i!) -\\lambda_k) $$\nAlgorithme EM\nOn choisit au hasard les $\\pi_k^{[0]}$, tels que $0\u0026lt;\\pi_k^{[0]}\u0026lt;1$ et $\\sum \\pi_k^{[0]} =1$ ainsi que les $\\lambda_k^{[0]}$ positifs\nOn choisit un $\\epsilon$ petit qui sera le seuil à partir du quel on cosidèrera que l\u0026rsquo;évolution est minime, et que l\u0026rsquo;on arrive ainsi au maximum\nSoit un critère d\u0026rsquo;arrêt de l\u0026rsquo;algorithme $l(\\theta^{[r]};x) - l(\\theta^{[r-1]};x)\\leq\\epsilon$\nA chaque étape\n  On détermine une estimation des $z_{ik}$, les $t_{ik}$ calculés à l\u0026rsquo;aide des apriori, cad les paramètres de l\u0026rsquo;itération précédente : $$ t_{ik}(\\theta^{[r-1]})=\\frac{\\pi_k^{[r-1]}p_k(x_i;\\lambda_k^{[r-1]})}{\\sum_l \\pi_l^{[r-1]}p_l(x_i;\\lambda_l^{[r-1]})} $$\n  On maximise la vraissemblance complétée que l\u0026rsquo;on sait désormais calculer $l(\\theta;x,z)$ non calculable car dépendant des $z_{ik}$ inconnus devient $l(\\theta;x,t(\\theta^{[r-1]}))$ avec $t(\\theta^{[r-1]})$ l\u0026rsquo;ensemble des $t_{ik}(\\theta^{[r-1]})$ Soit la recheche de $$ \\theta^{[r]}=\\arg\\max_{\\theta}l(\\theta;x,t(\\theta^{[r-1]}))=\\arg\\max_{\\theta}\\sum_{i=1}^n\\sum_{k=1}^nt_{ik}(\\theta^{[r-1]})ln( \\pi_k \\frac{\\lambda_k^{x_i}}{x_i!}e^{-\\lambda_k}) $$\n  Calcul des $\\lambda_k$\n$$ \\frac{\\partial}{\\partial \\lambda_k}l(\\theta;x,t(\\theta^{[r-1]}))= \\sum_i t_{ik}(\\theta^{[r-1]})\\frac{\\partial}{\\partial \\lambda_k}(ln(\\pi_k) -ln(x_i!)+x_iln(\\lambda_k)-\\lambda_k) $$ $$ = \\sum_i t_{ik}(\\theta^{[r-1]})(\\frac{x_i}{\\lambda_k}-1) $$ ⚠️ les $t_{ik}(\\theta^{[r-1]})$ ne dépendent pas de $\\lambda_k$ (mais de $\\lambda_k^{[r-1]})$ ! $$ \\frac{\\partial}{\\partial \\lambda_k}l(\\theta;x,t(\\theta^{[r-1]}))= 0 \\iff \\frac{1}{\\lambda_k^{[r]}}\\sum_i t_{ik}(\\theta^{[r-1]})x_i = \\sum_i t_{ik}(\\theta^{[r-1]}) $$ $$ \\iff \\lambda_k^{[r]} = \\frac{\\sum_i t_{ik}(\\theta^{[r-1]})x_i}{\\sum_i t_{ik}(\\theta^{[r-1]})} $$\nCalcul des $\\pi_k$\nIl s\u0026rsquo;agit d\u0026rsquo;une estimation sous contrainte ($\\sum_k \\pi_k =1$), on utilise donc un lagrangien\n$L(\\theta,\\mu)=l(\\theta;x, t_{ik}(\\theta^{[r-1]})) - \\mu (\\sum_k \\pi_k - 1)$ $$ \\frac{\\partial}{\\partial \\pi_k} L(\\theta) = \\sum_i t_{ik}(\\theta^{[r-1]}) \\frac{\\partial}{\\partial \\pi_k}(ln(\\pi_k) -ln(x_i!)+x_iln(\\lambda_k)-\\lambda_k) - \\mu $$ $$ = \\sum_i \\frac{t_{ik}(\\theta^{[r-1]}) }{\\pi_k} - \\mu $$ (dériver selon $\\mu$ fait simplement retrouver la contrainte $\\sum_k\\pi_k=1$) $$ \\frac{\\partial}{\\partial \\pi_k} L(\\theta) = 0 \\iff \\pi_k^{[r]}=\\frac{1}{\\mu}\\sum_i t_{ik}(\\theta^{[r-1]}) $$ puis on a par contrainte $$ \\sum_k \\pi_k^{[r]}=1=\\frac{1}{\\mu}\\sum_k\\sum_i t_{ik}(\\theta^{[r-1]}) $$ par construction $$ \\sum_k t_{ik}(\\theta^{[r-1]}) = 1 $$ donc $$ \\sum_k\\sum_i t_{ik}(\\theta^{[r-1]}) =n $$ soit $\\mu=n$ et $$ \\pi_k^{[r]}=\\frac{1}{n}\\sum_i t_{ik}(\\theta^{[r-1]}) $$ Au final étant donné que les $t_{ik}$ sont les espérances d\u0026rsquo;appartenance à chacune des classes, la somme est donc l\u0026rsquo;espérance de l\u0026rsquo;effectif de chacune des classes et on ne fait donc que calculer une fréquence (espérance du nombre d\u0026rsquo;individu dans la classe divisé par n) !\nItération suivante\nConnaissant les nouveaux paramètres de $\\theta$ on peut calculer la nouvelle valeur de la vraissemblance $l(\\theta^{[r]};x)$ que l\u0026rsquo;on compare à celle de l\u0026rsquo;itération précédente\nRemarque : algorithme CEM # C\u0026rsquo;est l\u0026rsquo;algorithme EM plus réaliste du point de vue de la classification. Au lieu de remplacer les $z_{ik}$ par leur espérance $t_{ik}$, on effectue la classification, c\u0026rsquo;est à dire que l\u0026rsquo;on affecte chaque $i$ dans la classe où il est le plus probable, cad vérifiant $\\max_kt_{ik}$ On remplace donc les $z_{ik}$ par des valeurs plus réelles, 1 si $i$ a été affecté à $k$ lors de l\u0026rsquo;itération $[r-1]$, 0 sinon. Dans l\u0026rsquo;algorithme CEM, les $\\pi_k^{[r]}$ sont les vrais proportions des classes formées à l\u0026rsquo;itération $[r-1]$\nRemarque : méthode aléatoire # Comme pour les k-means, la méthode peut aboutir à un maximum local. Il faut donc relancer plusieurs fois l\u0026rsquo;algorithme, et conserver le cas qui a donné le plus haut résultat de vraissemblance (le maximum des maximum locaux a des chances d\u0026rsquo;être le maximum global)\nRemarque : choisir le nombre de classes # On pourrait se dire que l\u0026rsquo;on choisit $K$ tel que le maximum de vraissemblance obtenu soit le meilleur possible. Mais comme avec l\u0026rsquo;inertie intra des méthodes géométriques, mécaniquement en augmentant K, le maximum de vraissemblance sera toujours plus élevé. En effet $K$ plus élevé signifie un plus grand nombre de paramètres, ce qui améliore forcément le modèle. On utilise au final des critères qui pénalisent le nombre de paramètre, AIC et BIC. C\u0026rsquo;est le même raisonnement que lorsqu\u0026rsquo;on observe un coude dans les gains d\u0026rsquo;inertie intra, on se demande si l\u0026rsquo;ajout de paramètre est \u0026ldquo;rentable\u0026rdquo;.\nExercice 3 : Lien entre clustering géométrique et probabiliste # On considère un échantillon $x=(x1,\u0026hellip;,xn)$ composé de $n$ observations indépendantes. Chaque observation $x_i∈\\mathbb{R}^d$ est décrite par $d$ variables continues. Montrer que le clustering de ces données en $K$ classes effectué par un algorithme des Kmeans avec la métrique diagonale $M=diag(\\frac{1}{s^2_1},\u0026hellip;,\\frac{1}{s^2_d})$ (métrique inverse des variances) est équivalent à un clustering effectué par un algorithme CEM considérant le modèle défini par la densité suivante :\n$$ p(x_i;\\theta) =\\sum_{k=1}^{K}\\frac{1}{K}\\prod_{j=1}^{d}\\phi(x_{ij};\\mu_{kj},s^2_j) $$ avec $$ \\phi(x_{ij};\\mu_{kj},s^2_j)=\\frac{1}{s_j\\sqrt{2\\pi}}e^{-\\frac{1}{2}(\\frac{x_{ij}-\\mu_{kj}}{s_j})^2} $$ la densité d\u0026rsquo;une loi normale d\u0026rsquo;espérance $\\mu_{kj}$ et de variance $s^2_j$\n Vision géométrique : voir le premier exercice du TP ou l\u0026rsquo;on a créé 4 groupes de points avec des moyennes différentes Vision probabiliste : on observe une ditribution avec $K$ \u0026ldquo;cloches\u0026rdquo; superposées  Clustering Kmeans Détermination de l\u0026rsquo;estimateur $(\\tilde z,\\tilde \\mu)$, où les $\\mu_k$ sont les centres obtenus et les $z_{ik}$ calculé selon les points dont $\\mu_k$ est le plus proche $$ (\\tilde z,\\tilde \\mu) = \\arg\\min_{z,\\mu} \\sum_i\\sum_k z_{ik} d^2_M(x_i,\\mu_k) $$ En effet on recherche la configuration selon laquelle l\u0026rsquo;inertie intra est la plus faible, càd telles que la somme de la somme des distances des points à leur baycentre est la plus faible $$ = \\arg\\min_{z,\\mu} \\sum_i\\sum_k z_{ik} \\sum_{j=1}^d\\frac{(x_{ij} -\\mu_{kj})^2}{s^2_j} $$ le barycentre de classe k étant le point de coordonnées $\\mu_k=(\\mu_{k,1},\u0026hellip;,\\mu_{k,d})$\nClustering CEM Détermination de l\u0026rsquo;estimateur $(\\hat z,\\hat \\mu)$ obtenu par maximum de vraissemblance $$ (\\hat z,\\hat \\mu)=\\arg\\max_{z,\\mu} \\sum_i\\sum_k z_{i_k}ln(\\frac{1}{K}\\prod_{j=1}^d\\phi(x_{ij};\\mu_{kj},s^2_j)) $$ $$ =\\arg\\max_{z,\\mu} \\sum_i\\sum_k z_{i_k}ln(\\frac{1}{K}) +z_{i_k} \\sum_{j=1}^dln(\\phi(x_{ij};\\mu_{kj},s^2_j)) $$ $$ =\\arg\\max_{z,\\mu} \\sum_i\\sum_k [ z_{i_k}ln(\\frac{1}{K}) - z_{i_k} \\frac{1}{2}\\sum_{j=1}^dln(2\\pi s_j^2) -z_{i_k} \\frac{1}{2} \\sum_{j=1}^d\\frac{(x_{ij}-\\mu_{kj})^2}{s^2_j} ] $$ $$ =\\arg\\max_{z,\\mu} nln(\\frac{1}{K}) - n\\frac{1}{2}\\sum_{j=1}^dln(2\\pi s_j^2) - \\sum_i\\sum_kz_{i_k} \\frac{1}{2} \\sum_{j=1}^d\\frac{(x_{ij}-\\mu_{kj})^2}{s^2_j} $$ $$ =\\arg\\max_{z,\\mu} - \\sum_i\\sum_kz_{i_k} \\sum_{j=1}^d\\frac{(x_{ij}-\\mu_{kj})^2}{s^2_j} $$ $$ =\\arg\\min_{z,\\mu} \\sum_i\\sum_kz_{i_k} \\sum_{j=1}^d\\frac{(x_{ij}-\\mu_{kj})^2}{s^2_j} $$\n"}).add({id:5,href:"/docs/ensai/classification/tp/",title:"TP Classification",description:"TP2 exo2\nrm(list = ls()) install.packages(\u0026quot;mclust\u0026quot;) require(cluster) require(pgmm) require(FactoMineR) require(explor) require(VarSelLCM) require(mclust) data(\u0026quot;coffee\u0026quot;) ?coffee summary(coffee) coffee$Variety \u0026lt;- as.factor(coffee$Variety) out.pca \u0026lt;- PCA(coffee, quali.sup = 1:2, graph = F) explor(out.pca) plot(out.pca$ind$coord[,1], out.pca$ind$coord[,2], col=coffee[,1]) ?VarSelCluster res.diago.bic \u0026lt;- VarSelCluster(coffee[, -c(1, 2)], 1:8, vbleSelec = F) summary(res.diago.bic) res.diago.icl \u0026lt;- VarSelCluster(coffee[, -c(1, 2)], 1:8, vbleSelec = F, crit.varsel = \u0026quot;ICL\u0026quot;) summary(res.diago.icl) res.diago.bic@partitions table(coffee[, 1], res.diago.bic@partitions@zMAP) table(coffee[, 2], res.diago.bic@partitions@zMAP) par(mfrow=c(1,1)) plot(out.pca$ind$coord[, 1], out.pca$ind$coord[, 2], col = res.diago.bic@partitions@zMAP) sil \u0026lt;- silhouette(res.",content:"TP2 exo2\nrm(list = ls()) install.packages(\u0026quot;mclust\u0026quot;) require(cluster) require(pgmm) require(FactoMineR) require(explor) require(VarSelLCM) require(mclust) data(\u0026quot;coffee\u0026quot;) ?coffee summary(coffee) coffee$Variety \u0026lt;- as.factor(coffee$Variety) out.pca \u0026lt;- PCA(coffee, quali.sup = 1:2, graph = F) explor(out.pca) plot(out.pca$ind$coord[,1], out.pca$ind$coord[,2], col=coffee[,1]) ?VarSelCluster res.diago.bic \u0026lt;- VarSelCluster(coffee[, -c(1, 2)], 1:8, vbleSelec = F) summary(res.diago.bic) res.diago.icl \u0026lt;- VarSelCluster(coffee[, -c(1, 2)], 1:8, vbleSelec = F, crit.varsel = \u0026quot;ICL\u0026quot;) summary(res.diago.icl) res.diago.bic@partitions table(coffee[, 1], res.diago.bic@partitions@zMAP) table(coffee[, 2], res.diago.bic@partitions@zMAP) par(mfrow=c(1,1)) plot(out.pca$ind$coord[, 1], out.pca$ind$coord[, 2], col = res.diago.bic@partitions@zMAP) sil \u0026lt;- silhouette(res.diago.bic@partitions@zMAP, daisy(coffee[, -c(1, 2)])) sil \u0026lt;- silhouette(res.diago.icl@partitions@zMAP, daisy(coffee[, -c(1, 2)])) plot(sil) summary(sil) data.with.partitions \u0026lt;- cbind(coffee, factor(res.diago.bic@partitions@zMAP)) ?catdes catdes(data.with.partitions, ncol(data.with.partitions)) by(coffee, res.diago.icl@partitions@zMAP, summary) res.diago.icl@criteria@discrim cor(coffee[res.diago.bic@partitions@zMAP == 1, -c(1, 2)]) cor(coffee[res.diago.bic@partitions@zMAP == 2, -c(1, 2)]) ?Mclust mclust.options(\u0026quot;emModelNames\u0026quot;) resfull \u0026lt;- Mclust(coffee[, -c(1, 2)], 1:8) summary(resfull) resfull$parameters table(resfull$classification, res.diago.bic@partitions@zMAP)  TP2 exo1\n############################################################################################################## # Mélange de distributions de poissons ############################################################################################################## rm(list = ls()) rdata \u0026lt;- function(n, prop, lambda) { g \u0026lt;- length(prop) z \u0026lt;- sample(1:g, n, replace = TRUE, prob = prop) x \u0026lt;- rpois(n, lambda[z]) list(z = z, x = x) } oneEM \u0026lt;- function(x, g, tol) { # Init prop \u0026lt;- runif(g) prop \u0026lt;- prop / sum(prop) lambda \u0026lt;- sample(x, g) # Calcul loglike masse.cond \u0026lt;- sapply(1:g, function(k) dpois(x, lambda[k]) * prop[k]) loglike \u0026lt;- sum(log(rowSums(masse.cond))) prec \u0026lt;- -Inf while ((loglike - prec) \u0026gt; tol) { # Estep tik \u0026lt;- masse.cond / rowSums(masse.cond) # Mstep prop \u0026lt;- colSums(tik) / sum(tik) lambda \u0026lt;- as.numeric(t(tik) %*% x) / colSums(tik) # Calcul loglike masse.cond \u0026lt;- sapply(1:g, function(k) dpois(x, lambda[k]) * prop[k]) prec \u0026lt;- loglike loglike \u0026lt;- sum(log(rowSums(masse.cond))) } list( prop = prop, lambda = lambda, loglike = loglike, tik = tik, check = loglike \u0026gt; prec, masse.cond = masse.cond ) } multi.EM \u0026lt;- function(x, g, nbinit = 20, tol = 0.01) { res \u0026lt;- replicate(nbinit, oneEM(x, g, tol), simplify = FALSE) res \u0026lt;- res[[which.max(sapply(res, function(u) u$loglike))]] res } MAPassign \u0026lt;- function(prob) { as.factor(apply(prob, 1, which.max)) } estim.mixture \u0026lt;- function(x, gmax, criterion = \u0026quot;BIC\u0026quot;, nbinit = 20, tol = 0.01) { results \u0026lt;- lapply(1:gmax, multi.EM, x = x, nbinit = nbinit, tol = tol) for (g in 1:gmax) { results[[g]]$partition \u0026lt;- MAPassign(results[[g]]$tik) results[[g]]$BIC \u0026lt;- results[[g]]$loglike - (2 * g - 1) * 0.5 * log(length(x)) results[[g]]$ICL \u0026lt;- sum(log(apply(results[[g]]$masse.cond, 1, max))) - (2 * g - 1) * 0.5 * log(length(x)) } if (criterion == \u0026quot;BIC\u0026quot;) { results \u0026lt;- results[[which.max(sapply(results, function(u) u$BIC))]] } else if (criterion == \u0026quot;ICL\u0026quot;) { results \u0026lt;- results[[which.max(sapply(results, function(u) u$ICL))]] } results } set.seed(123) ech \u0026lt;- rdata(100, c(1 / 2, 1 / 2), c(5, 20)) solution \u0026lt;- estim.mixture(ech$x, 8) solution$prop solution$lambda table(solution$partition, ech$z)  TP2 exo3\n############################################################################################################## # Sélection de variable et gestion de données manquantes ############################################################################################################## rm(list = ls()) require(VarSelLCM) require(missMDA) generdata \u0026lt;- function(n, r, d, epsilon) { z \u0026lt;- sample(1:2, n, replace = TRUE) x \u0026lt;- matrix(rnorm(n * r), n, r) x[which(z == 1), ] \u0026lt;- x[which(z == 1), ] + epsilon x[which(z == 2), ] \u0026lt;- x[which(z == 2), ] - epsilon x \u0026lt;- cbind(x, matrix(rnorm(n * (d - r), sd = sqrt(2)), n, d - r)) list(z = z, x = x) } giveARI \u0026lt;- function(ech) { res.with \u0026lt;- VarSelCluster(ech$x, 2, vbleSelec = TRUE, crit.varsel = \u0026quot;BIC\u0026quot;) res.without \u0026lt;- VarSelCluster(ech$x, 2, vbleSelec = FALSE) c(ARI(ech$z, res.with@partitions@zMAP), ARI(ech$z, res.without@partitions@zMAP)) } set.seed(123) all.ech \u0026lt;- replicate(20, generdata(100, 3, 20, 1), simplify = FALSE) res \u0026lt;- sapply(all.ech, giveARI) summary(t(res)) generdata2 \u0026lt;- function(n, r, d, epsilon, tau) { z \u0026lt;- sample(1:2, n, replace = TRUE) x \u0026lt;- matrix(rnorm(n * r), n, r) x[which(z == 1), ] \u0026lt;- x[which(z == 1), ] + epsilon x[which(z == 2), ] \u0026lt;- x[which(z == 2), ] - epsilon x \u0026lt;- cbind(x, matrix(rnorm(n * (d - r), sd = sqrt(2)), n, d - r)) x.notna \u0026lt;- x for (j in 1:ncol(x)) { naloc \u0026lt;- which(runif(n) \u0026lt; tau) if (length(naloc) \u0026gt; 0) { x[naloc, j] \u0026lt;- NA } } list(z = z, x = x, x.notna = x.notna) } giveARI.na \u0026lt;- function(ech) { x.impute \u0026lt;- imputePCA(ech$x, scale = FALSE)$completeObs ari.geo \u0026lt;- ARI(ech$z, kmeans(x.impute, 2)$cluster) ari.mixture \u0026lt;- ARI(ech$z, VarSelCluster(ech$x, 2, vbleSelec = TRUE)@partitions@zMAP) c(ari.geo, ari.mixture) } set.seed(123) all.ech \u0026lt;- replicate(20, generdata2(101, 3, 6, 1, .2), simplify = FALSE) res \u0026lt;- sapply(all.ech, giveARI.na) summary(t(res)) generdata3 \u0026lt;- function(n, r, d, epsilon, tau) { z \u0026lt;- sample(1:2, n, replace = TRUE) x \u0026lt;- matrix(rnorm(n * r), n, r) x[which(z == 1), ] \u0026lt;- x[which(z == 1), ] + epsilon x[which(z == 2), ] \u0026lt;- x[which(z == 2), ] - epsilon x \u0026lt;- cbind(x, matrix(rnorm(n * (d - r), sd = sqrt(2)), n, d - r)) x.notna \u0026lt;- x for (j in 1:ncol(x)) { naloc \u0026lt;- which(x[, j] \u0026lt; tau) if (length(naloc) \u0026gt; 0) { x[naloc, j] \u0026lt;- NA } } list(z = z, x = x, x.notna = x.notna) } set.seed(123) all.ech \u0026lt;- replicate(20, generdata3(101, 3, 6, 1, 1), simplify = FALSE) res \u0026lt;- sapply(all.ech, giveARI.na) summary(t(res))  "}).add({id:6,href:"/docs/ensai/classification/",title:"Classification",description:"TDs et TPs",content:""}).add({id:7,href:"/docs/",title:"Docs",description:"",content:""}).add({id:8,href:"/docs/dev/",title:"Développement",description:"",content:""}).add({id:9,href:"/docs/ensai/",title:"Ensai",description:"TDs et TPs",content:""}).add({id:10,href:"/docs/ensai/projet-ensai/",title:"Projet statistique",description:"",content:""}).add({id:11,href:"/docs/ensai/seub/",title:"Statistique descriptive",description:"",content:""}),search.addEventListener("input",t,!0);function t(){const s=5;var n=this.value,o=e.search(n,{limit:s,enrich:!0});const t=new Map;for(const e of o.flatMap(e=>e.result)){if(t.has(e.doc.href))continue;t.set(e.doc.href,e.doc)}if(suggestions.innerHTML="",suggestions.classList.remove("d-none"),t.size===0&&n){const e=document.createElement("div");e.innerHTML=`No results for "<strong>${n}</strong>"`,e.classList.add("suggestion__no-results"),suggestions.appendChild(e);return}for(const[r,a]of t){const n=document.createElement("div");suggestions.appendChild(n);const e=document.createElement("a");e.href=r,n.appendChild(e);const o=document.createElement("span");o.textContent=a.title,o.classList.add("suggestion__title"),e.appendChild(o);const i=document.createElement("span");if(i.textContent=a.description,i.classList.add("suggestion__description"),e.appendChild(i),suggestions.appendChild(n),suggestions.childElementCount==s)break}}})()