<!doctype html><html lang=fr-fr><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="ie=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><link rel=preload as=font href=https://formation.dufau.re/fonts/vendor/jost/jost-v4-latin-regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://formation.dufau.re/fonts/vendor/jost/jost-v4-latin-500.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://formation.dufau.re/fonts/vendor/jost/jost-v4-latin-700.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://formation.dufau.re/fonts/KaTeX_Main-Regular.woff2 type=font/woff2 crossorigin><link rel=preload as=font href=https://formation.dufau.re/fonts/KaTeX_Math-Italic.woff2 type=font/woff2 crossorigin><script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script><link rel=stylesheet href=https://formation.dufau.re/main.0625e80f000d9d96c1e7ce53cd500c0d6092e519368fbf5e075488c71337d4b97e2c387333fae078224cb7f952069e8d5c99e3a6b749bb677f48575e40d30ebe.css integrity="sha512-BiXoDwANnZbB585TzVAMDWCS5Rk2j79eB1SIxxM31Ll+LDhzM/rgeCJMt/lSBp6NXJnjprdJu2d/SFdeQNMOvg==" crossorigin=anonymous><noscript><style>img.lazyload{display:none}</style></noscript><meta name=robots content="index, follow"><meta name=googlebot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><meta name=bingbot content="index, follow, max-snippet:-1, max-image-preview:large, max-video-preview:-1"><title>TD2 Classification - Formations</title><meta name=description content="Méthodes probabilistes"><link rel=canonical href=https://formation.dufau.re/docs/ensai/classification/td2/><meta property="og:locale" content="en_US"><meta property="og:type" content="article"><meta property="og:title" content="TD2 Classification"><meta property="og:description" content="Méthodes probabilistes"><meta property="og:url" content="https://formation.dufau.re/docs/ensai/classification/td2/"><meta property="og:site_name" content="Formations"><meta property="og:image" content="https://formation.dufau.re/doks.png"><meta property="og:image:alt" content="Formations"><meta name=twitter:card content="summary_large_image"><meta name=twitter:site content="@getdoks"><meta name=twitter:creator content="@henkverlinde"><meta name=twitter:title content="TD2 Classification"><meta name=twitter:description content="Méthodes probabilistes"><meta name=twitter:image content="https://formation.dufau.re/doks.png"><meta name=twitter:image:alt content="TD2 Classification"><script type=application/ld+json>{"@context":"https://schema.org","@graph":[{"@type":"Organization","@id":"https://formation.dufau.re/#/schema/organization/1","name":"Doks","url":"https://formation.dufau.re/","sameAs":["https://twitter.com/getdoks","https://github.com/h-enk/doks"],"logo":{"@type":"ImageObject","@id":"https://formation.dufau.re/#/schema/image/1","url":"https://formation.dufau.re/logo-doks.png","width":512,"height":512,"caption":"Doks"},"image":{"@id":"https://formation.dufau.re/#/schema/image/1"}},{"@type":"WebSite","@id":"https://formation.dufau.re/#/schema/website/1","url":"https://formation.dufau.re/","name":"Formations","description":"Doks is a Hugo theme for building secure, fast, and SEO-ready documentation websites, which you can easily update and customize.","publisher":{"@id":"https://formation.dufau.re/#/schema/organization/1"}},{"@type":"WebPage","@id":"https://formation.dufau.re/docs/ensai/classification/td2/","url":"https://formation.dufau.re/docs/ensai/classification/td2/","name":"TD2 Classification","description":"Méthodes probabilistes","isPartOf":{"@id":"https://formation.dufau.re/#/schema/website/1"},"about":{"@id":"https://formation.dufau.re/#/schema/organization/1"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","breadcrumb":{"@id":"https://formation.dufau.re/docs/ensai/classification/td2/#/schema/breadcrumb/1"},"primaryImageOfPage":{"@id":"https://formation.dufau.re/docs/ensai/classification/td2/#/schema/image/2"},"inLanguage":"en-US","potentialAction":[{"@type":"ReadAction","target":["https://formation.dufau.re/docs/ensai/classification/td2/"]}]},{"@type":"BreadcrumbList","@id":"https://formation.dufau.re/docs/ensai/classification/td2/#/schema/breadcrumb/1","name":"Breadcrumbs","itemListElement":[{"@type":"ListItem","position":1,"item":{"@type":"WebPage","@id":"https://formation.dufau.re/","url":"https://formation.dufau.re/","name":"Home"}},{"@type":"ListItem","position":2,"item":{"@type":"WebPage","@id":"https://formation.dufau.re/docs/","url":"https://formation.dufau.re/docs/","name":"Docs"}},{"@type":"ListItem","position":3,"item":{"@type":"WebPage","@id":"https://formation.dufau.re/docs/ensai/","url":"https://formation.dufau.re/docs/ensai/","name":"Ensai"}},{"@type":"ListItem","position":4,"item":{"@type":"WebPage","@id":"https://formation.dufau.re/docs/ensai/classification/","url":"https://formation.dufau.re/docs/ensai/classification/","name":"Classification"}},{"@type":"ListItem","position":5,"item":{"@id":"https://formation.dufau.re/docs/ensai/classification/td2/"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Article","@id":"https://formation.dufau.re/#/schema/article/1","headline":"TD2 Classification","description":"Méthodes probabilistes","isPartOf":{"@id":"https://formation.dufau.re/docs/ensai/classification/td2/"},"mainEntityOfPage":{"@id":"https://formation.dufau.re/docs/ensai/classification/td2/"},"datePublished":"0001-01-01T00:00:00CET","dateModified":"0001-01-01T00:00:00CET","author":{"@id":"https://formation.dufau.re/#/schema/person/2"},"publisher":{"@id":"https://formation.dufau.re/#/schema/organization/1"},"image":{"@id":"https://formation.dufau.re/docs/ensai/classification/td2/#/schema/image/2"}}]},{"@context":"https://schema.org","@graph":[{"@type":"Person","@id":"https://formation.dufau.re/#/schema/person/2","name":"Henk Verlinde","sameAs":["https://twitter.com/henkverlinde","https://www.linkedin.com/in/henkverlinde/","https://github.com/h-enk"]}]},{"@context":"https://schema.org","@graph":[{"@type":"ImageObject","@id":"https://formation.dufau.re/docs/ensai/classification/td2/#/schema/image/2","url":"https://formation.dufau.re/doks.png","contentUrl":"https://formation.dufau.re/doks.png","caption":"TD2 Classification"}]}]}</script><meta name=theme-color content="#fff"><link rel=apple-touch-icon sizes=180x180 href=https://formation.dufau.re/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://formation.dufau.re/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://formation.dufau.re/favicon-16x16.png><link rel=manifest crossorigin=use-credentials href=https://formation.dufau.re/site.webmanifest></head><body class="docs single"><div class=header-bar></div><header class="navbar navbar-expand-md navbar-light doks-navbar"><nav class="container-xxl flex-wrap flex-md-nowrap" aria-label="Main navigation"><a class="navbar-brand p-0 me-auto" href=/ aria-label=Formations>Formations</a>
<button class="btn btn-menu d-block d-md-none order-5" type=button data-bs-toggle=offcanvas data-bs-target=#offcanvasDoks aria-controls=offcanvasDoks aria-label="Open main menu"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-menu"><line x1="3" y1="12" x2="21" y2="12"/><line x1="3" y1="6" x2="21" y2="6"/><line x1="3" y1="18" x2="21" y2="18"/></svg></button><div class="offcanvas offcanvas-end border-0 py-md-1" tabindex=-1 id=offcanvasDoks data-bs-backdrop=true aria-labelledby=offcanvasDoksLabel><div class="header-bar d-md-none"></div><div class="offcanvas-header d-md-none"><h2 class="h5 offcanvas-title ps-2" id=offcanvasDoksLabel><a class=text-dark href=/>Formations</a></h2><button type=button class="btn-close text-reset me-2" data-bs-dismiss=offcanvas aria-label="Close main menu"></button></div><div class="offcanvas-body px-4"><h3 class="h6 text-uppercase mb-3 d-md-none">Main</h3><ul class="nav flex-column flex-md-row ms-md-n3"><li class="nav-item dropdown"><a class="nav-link dropdown-toggle ps-0 py-1" href=# id=navbarDropdownMenuLink role=button data-bs-toggle=dropdown aria-expanded=false>Ensai
<span class=dropdown-caret><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-chevron-down"><polyline points="6 9 12 15 18 9"/></svg></span></a><ul class="dropdown-menu dropdown-menu-main shadow rounded border-0" aria-labelledby=navbarDropdownMenuLink><li><a class=dropdown-item href=/docs/ensai/classification>Classification</a></li><li><a class=dropdown-item href=/docs/ensai/projet-ensai>Projet stat</a></li><li><a class=dropdown-item href=/docs/ensai/sem>SEM</a></li></ul></li><li class=nav-item><a class="nav-link ps-0 py-1" href=/docs/dev/>Développement</a></li></ul><hr class="text-black-50 my-4 d-md-none"><h3 class="h6 text-uppercase mb-3 d-md-none">Socials</h3><ul class="nav flex-column flex-md-row ms-md-auto me-md-n5 pe-md-2"><li class=nav-item><a class="nav-link ps-0 py-1" href=https://github.com/clement-dufaure><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37.0 00-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44.0 0020 4.77 5.07 5.07.0 0019.91 1S18.73.65 16 2.48a13.38 13.38.0 00-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07.0 005 4.77 5.44 5.44.0 003.5 8.55c0 5.42 3.3 6.61 6.44 7A3.37 3.37.0 009 18.13V22"/></svg><small class="ms-2 d-md-none">GitHub</small></a></li></ul></div></div><button id=mode class="btn btn-link order-md-1" type=button aria-label="Toggle user interface mode">
<span class=toggle-dark><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg></span><span class=toggle-light><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></span></button></nav></header><nav class="doks-subnavbar py-2 sticky-lg-top" aria-label="Secondary navigation"><div class="container-xxl d-flex align-items-md-center"><form class="doks-search position-relative flex-grow-1 me-auto"><input id=search class="form-control is-search" type=search placeholder="Search docs..." aria-label="Search docs..." autocomplete=off><div id=suggestions class="shadow bg-white rounded d-none"></div></form><button class="btn doks-sidebar-toggle d-lg-none ms-3 order-3 collapsed" type=button data-bs-toggle=collapse data-bs-target=#doks-docs-nav aria-controls=doks-docs-nav aria-expanded=false aria-label="Toggle documentation navigation"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-expand" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Expand</title><polyline points="7 13 12 18 17 13"/><polyline points="7 6 12 11 17 6"/></svg><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" class="doks doks-collapse" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><title>Collapse</title><polyline points="17 11 12 6 7 11"/><polyline points="17 18 12 13 7 18"/></svg></button></div></nav><div class=container-xxl><aside class=doks-sidebar><nav id=doks-docs-nav class="collapse d-lg-none" aria-label="Tertiary navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6d946b37f274d19dc18b6f8e8f7eb8b6 aria-expanded=true>
Ensai</button><div class="collapse show" id=section-6d946b37f274d19dc18b6f8e8f7eb8b6><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-5c9335b28e6277ddbe0137f04e5e58a7 aria-expanded=true>
Classification</button><div class="collapse show" id=section-5c9335b28e6277ddbe0137f04e5e58a7><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/classification/td1/>TD1 Classification</a></li><li><a class="docs-link rounded active" href=https://formation.dufau.re/docs/ensai/classification/td2/>TD2 Classification</a></li><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/classification/tp/>TP Classification</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-54d1ba85f1608406259034bb3144afe3 aria-expanded=false>
Projet statistique</button><div class=collapse id=section-54d1ba85f1608406259034bb3144afe3><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/projet-ensai/projet-ensai/>Projet statistique</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e96164fdba9ca516ca6255d35148cf90 aria-expanded=false>
Statistique descriptive</button><div class=collapse id=section-e96164fdba9ca516ca6255d35148cf90><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/seub/td/>TD1</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-b7b26e69e6255b6d8955888500608535 aria-expanded=false>
Développement</button><div class=collapse id=section-b7b26e69e6255b6d8955888500608535><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/dev/securite-applicative/>Sécurité applicative</a></li></ul></div></li></ul></nav></aside></div><div class="wrap container-xxl" role=document><div class=content><div class="row flex-xl-nowrap"><div class="col-lg-5 col-xl-4 docs-sidebar d-none d-lg-block"><nav class=docs-links aria-label="Main navigation"><ul class="list-unstyled collapsible-sidebar"><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-6d946b37f274d19dc18b6f8e8f7eb8b6 aria-expanded=true>
Ensai</button><div class="collapse show" id=section-6d946b37f274d19dc18b6f8e8f7eb8b6><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-5c9335b28e6277ddbe0137f04e5e58a7 aria-expanded=true>
Classification</button><div class="collapse show" id=section-5c9335b28e6277ddbe0137f04e5e58a7><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/classification/td1/>TD1 Classification</a></li><li><a class="docs-link rounded active" href=https://formation.dufau.re/docs/ensai/classification/td2/>TD2 Classification</a></li><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/classification/tp/>TP Classification</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-54d1ba85f1608406259034bb3144afe3 aria-expanded=false>
Projet statistique</button><div class=collapse id=section-54d1ba85f1608406259034bb3144afe3><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/projet-ensai/projet-ensai/>Projet statistique</a></li></ul></div></li><li class="my-1 ms-3"><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-e96164fdba9ca516ca6255d35148cf90 aria-expanded=false>
Statistique descriptive</button><div class=collapse id=section-e96164fdba9ca516ca6255d35148cf90><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/ensai/seub/td/>TD1</a></li></ul></div></li></ul></div></li><li class=mb-1><button class="btn btn-toggle align-items-center rounded collapsed" data-bs-toggle=collapse data-bs-target=#section-b7b26e69e6255b6d8955888500608535 aria-expanded=false>
Développement</button><div class=collapse id=section-b7b26e69e6255b6d8955888500608535><ul class="btn-toggle-nav list-unstyled fw-normal pb-1 small"><li><a class="docs-link rounded" href=https://formation.dufau.re/docs/dev/securite-applicative/>Sécurité applicative</a></li></ul></div></li></ul></nav></div><nav class="docs-toc d-none d-xl-block col-xl-3" aria-label="Secondary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#préambule>Préambule</a></li><li><a href=#lalgorithme-em>L&rsquo;algorithme EM</a></li><li><a href=#convergence-de-lalgorithme>Convergence de l&rsquo;algorithme</a></li></ul><ul><li><a href=#estimation-par-maximum-de-vraissemblance>Estimation par maximum de vraissemblance</a><ul><li><a href=#remarque--algorithme-cem>Remarque : algorithme CEM</a></li><li><a href=#remarque--méthode-aléatoire>Remarque : méthode aléatoire</a></li><li><a href=#remarque--choisir-le-nombre-de-classes>Remarque : choisir le nombre de classes</a></li></ul></li></ul></nav></div></nav><main class="docs-content col-lg-11 col-xl-9"><h1>TD2 Classification</h1><p class=lead></p><nav class=d-xl-none aria-label="Quaternary navigation"><div class=page-links><h3>On this page</h3><nav id=TableOfContents><ul><li><a href=#préambule>Préambule</a></li><li><a href=#lalgorithme-em>L&rsquo;algorithme EM</a></li><li><a href=#convergence-de-lalgorithme>Convergence de l&rsquo;algorithme</a></li></ul><ul><li><a href=#estimation-par-maximum-de-vraissemblance>Estimation par maximum de vraissemblance</a><ul><li><a href=#remarque--algorithme-cem>Remarque : algorithme CEM</a></li><li><a href=#remarque--méthode-aléatoire>Remarque : méthode aléatoire</a></li><li><a href=#remarque--choisir-le-nombre-de-classes>Remarque : choisir le nombre de classes</a></li></ul></li></ul></nav></div></nav><h1 id=exercice-1--mélanges-et-algorithme-em>Exercice 1 : Mélanges et algorithme EM <a href=#exercice-1--m%c3%a9langes-et-algorithme-em class=anchor aria-hidden=true>#</a></h1><h2 id=préambule>Préambule <a href=#pr%c3%a9ambule class=anchor aria-hidden=true>#</a></h2><ul><li>Qu&rsquo;est ce qu&rsquo;un &ldquo;mélange&rdquo; de lois ?</li></ul><p>On considère un échantillon $x= (x1,&mldr;,xn)$ composé de $n$ observations indépendantes $x_i∈X$. Chaque observation est issue d’un mélange à $K$ composantes déterminées par $p(x_i;\theta)=\sum_{k=1}^{K}\pi_kp_k(x_i;\alpha_k)$,avec $\theta \in \Theta$.</p><p><figure class=figure><img class="figure-img img-fluid lazyload blur-up" data-sizes=auto src=https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_20x0_resize_box_3.png data-srcset="https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_900x0_resize_box_3.png 900w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_800x0_resize_box_3.png 800w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_700x0_resize_box_3.png 700w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_600x0_resize_box_3.png 600w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_500x0_resize_box_3.png 500w" width=626 height=382 alt=Image><noscript><img class="figure-img img-fluid" sizes=100vw srcset="https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_900x0_resize_box_3.png 900w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_800x0_resize_box_3.png 800w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_700x0_resize_box_3.png 700w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_600x0_resize_box_3.png 600w,https://formation.dufau.re/docs/ensai/classification/td2/melange1_hu89fe80c1e7ec12d0e8ad64c0126c6db6_2971_500x0_resize_box_3.png 500w" src=https://formation.dufau.re/docs/ensai/classification/td2/melange1.png width=626 height=382 alt=Image></noscript><figcaption class=figure-caption>=> loi inconnue compliquée à décrire, à paramétrer</figcaption></figure></p><p><figure class=figure><img class="figure-img img-fluid lazyload blur-up" data-sizes=auto src=https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_20x0_resize_box_3.png data-srcset="https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_900x0_resize_box_3.png 900w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_800x0_resize_box_3.png 800w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_700x0_resize_box_3.png 700w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_600x0_resize_box_3.png 600w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_500x0_resize_box_3.png 500w" width=626 height=382 alt=Image><noscript><img class="figure-img img-fluid" sizes=100vw srcset="https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_900x0_resize_box_3.png 900w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_800x0_resize_box_3.png 800w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_700x0_resize_box_3.png 700w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_600x0_resize_box_3.png 600w,https://formation.dufau.re/docs/ensai/classification/td2/melange2_hub5cfadef6f705d2bd5903e355da52928_8162_500x0_resize_box_3.png 500w" src=https://formation.dufau.re/docs/ensai/classification/td2/melange2.png width=626 height=382 alt=Image></noscript><figcaption class=figure-caption>=> 2 lois normales superposées</figcaption></figure></p><p>Idée :</p><ul><li>chaque point a une probabilité $\pi_k$ d&rsquo;appartenir à une des deux courbes (= proportion des points correspondant à chacune des 2 courbes, bien entendue inconnue), $\sum \pi_k =1$</li><li>la loi de chaque courbe est $p_k(x_i;\alpha_k)$ avec $\alpha_k$ les paramètres de la loi $p_k$ (si $p_k$ est une loi normale $\alpha_k=(\mu_k,\sigma_k)$)</li><li>Ce qui donne en additionnant chaque loi probable $p(x_i;\theta)=\sum_{k=1}^{K}\pi_kp_k(x_i;\alpha_k)$</li></ul><p>a)</p><ul><li>log-vraisemblance $l(\theta;x)$</li></ul><p>Vraissemblance : $L(\theta,x)=\prod_{i=1}^np(x_i;\theta)$</p><p>$l(\theta;x)$ = $\sum_{i=1}^{n}ln(p(x_i;\theta))$
= $\sum_{i=1}^{n}ln(\sum_{k=1}^{K}\pi_kp_k(x_i;\alpha_k))$</p><p>On obtient une équation sous une forme difficilement estimable (même par optimisation par un logiciel)</p><ul><li>log-vraisemblance complétée $l(\theta;x,z)$</li></ul><p>Idée : si on sait à quelle classe appartient chaque point, la loi est beaucoup plus simple</p><p>On introduit la variable catégorielle de l&rsquo;appartenance à la classe : $Classe_i=k$
Pour rester numérique on choisit de la noter avec des variables booléenne $Z_k$, telle que $z_{i,k}=1$ si $Classe_i=k$, sinon $z_{i,k}=0$</p><p>Pour le point $i$, on ne retient plus que la composante auquel il appartient</p><p>La &ldquo;contribution&rdquo; à la log-vraissembalnce du point $i$ appartenant à la classe k_0 est donc :
$$
ln(\pi_{k_0}p_{k_0}(x_i;\alpha_{k_0}))=z_{i_{k_0}}ln(\pi_{k_0}p_{k_0}(x_i;\alpha_{k_0}))
$$
car $z_{i_{k_0}}=1$
$$
=\sum_{k=1}^{K}z_{i,k}ln(\pi_kp_k(x_i;\alpha_k))
$$
car $z_{i_{k}}=0, \forall k\neq k_0$</p><p>soit</p><p>$l(\theta;x,z)=\sum_{i=1}^{n}\sum_{k=1}^{K}z_{i,k}ln(\pi_kp_k(x_i;\alpha_k))$</p><p>le modèle reste complexe, mais peut être résolu par l&rsquo;algorithme EM qui va avoir le bon gout de nous donner une estimation des $Z_k$ (et donc une classification)</p><p>b)</p><p>$t_{i,k}(\theta)$ probabilité <em>a posteriori</em> sous le paramètre $\theta$ que l’observation $i$ soit issue de la composante $k$.
⚠️ les $\pi_k$ sont les probabilité <em>a priori</em> et <strong>font partie</strong> de $\theta$ !!!</p><p>Calcul de probabilité à posteriori (Bayes)
$$
t_{i,k}(\theta)=P_\theta(z_{i,k}=1|X_i=x_i)=\frac{P_\theta(z_{i,k}=1)P_\theta(X_i=x_i|z_{i,k}=1)}{P_\theta(X_i=x_i)}
=\frac{\pi_kp_k(x_i;\alpha_k)}{p(x_i;\theta)}
$$</p><p>=> $t_{i,k}$ est donc l&rsquo;espérance conditionnelle de $z_{i,k}$ sachant $x_i$ (espérance d&rsquo;un Bernoulli B = P(B=1))</p><p>=> pour un i donné, la meilleure espérance parmi les k nous donne la classe la plus probable. Donc non seulement on va obtenir une classification, mais en plus on sera capable de mesurer à quel point un point est vraiment sûr dans une classe ou en ballotage (cas avec des probas 0.9/0.1 VS probas 0.49/0.51)</p><p>c)</p><p>Soit $E(t(\theta),z)=-\sum_{i=1}^{n}\sum_{k=1}^Kz_{i,k}ln(t_{i,k}(\theta))$,
avec $t(\theta) = (t_{i,k}(\theta);i= 1,&mldr;,n;k= 1,&mldr;,K)$ (vecteur des probabilités a posteriori)</p><p>Vérifier la formule d’Hathaway :
$l(\theta;x) =l(\theta;x,z) +E(t(\theta),z)$</p><p>À partir du calcul de $t_{i_k}$ précédent, que l&rsquo;on passe au logarithme :
$ln(t_{i,k}(\theta))=ln(\pi_kp_k(x_i;\alpha_k))-ln(p(x_i;\theta))$</p><p>On écrit l&rsquo;équation $K$ fois en multipliant par $z_{i,k}$ chaque ligne, et on somme les lignes</p><p>$$
\implies \sum_{k=1}^Kz_{ik}ln(t_{i,k}(\theta))
=\sum_{k=1}^Kz_{ik}ln(\pi_kp_k(x_i;\alpha_k))-ln(p(x_i;\theta))\sum_{k=1}^Kz_{i_k}
$$
($ln(p(x_i;\theta))$ ne dépend pas de $k$)</p><p>$\iff ln(p(x_i;\theta)) = \sum_{k=1}^Kz_{i,k}ln(\pi_kp_k(x_i;\alpha_k)) - \sum_{k=1}^Kz_{i,k}ln(t_{i,k}(\theta))$</p><p>Soit en sommant sur i :
$l(\theta;x) = l(\theta,x,z) + E(t(\theta),z)$</p><h2 id=lalgorithme-em>L&rsquo;algorithme EM <a href=#lalgorithme-em class=anchor aria-hidden=true>#</a></h2><p>EM = espérance - maximisation</p><p>Paramétrage &ldquo;fixe&rdquo; : le nombre de classe K</p><p>Initialisation : choix d&rsquo;un paramètre $\theta^{[0]}$ : $\theta$ est constitué des $\pi_k$ + des vecteurs $\alpha_k$ de chaque loi constituant le mélange
Exemple : mélange de 3 lois normales : 9 paramètres, les 3 proportions de classes + la moyenne et la variance de chaque classe (en réalité 8 paramètres car la dernière proportion est contrainte par 100-les deux premières)</p><p>Sur une itération $r$</p><ul><li><p>l&rsquo;étape d&rsquo;espérance : calcul de la logvraissamblance complété passant par le calcul des $t_{i_k}(\theta^{[r-1]})$
En effet la log vraissemblance complété vaut
$l(\theta;x,z)=\sum_{i=1}^{n}\sum_{k=1}^{K}z_{i,k}ln(\pi_kp_k(x_i;\alpha_k))$, comme on ne connait pas Z, on remplace par son espérance qui est donc $t_{i_k}$</p></li><li><p>l&rsquo;étape de maximisation : on maximise l&rsquo;espérance de la vraissamblance complétée, c&rsquo;est à dire recherche du nouveau $\theta^{[r]}$ qui maximise $\sum_i\sum_kt_{i,k}(\theta^{[r-1]})ln(\pi_kp_k(x_i;\alpha_k))$</p></li></ul><p>On itère jusqu&rsquo;à stabilisation de la vraissemblance</p><p>⚠️ initialisation aléatoire => possibilité de tomber sur un maximum local => lancer l&rsquo;algorithme pusieurs fois et conserver le meilleur résultat</p><h2 id=convergence-de-lalgorithme>Convergence de l&rsquo;algorithme <a href=#convergence-de-lalgorithme class=anchor aria-hidden=true>#</a></h2><p>$\Theta$ est l&rsquo;espace des paramètres possibles</p><p>Soit $\theta^{[r]} \in \Theta,\theta^{[r-1]} \in \Theta$, montrer que
$$
l(\theta^{[r]};x)=Q(\theta^{[r]};\theta^{[r-1]}) - H( \theta^{[r]};\theta^{[r-1]})
$$
avec
$$
Q(\theta^{[r]};\theta^{[r-1]}) =E_{\theta^{[r-1]}}[l(\theta^{[r]};x,z)|X_i=x_i]
$$
$$
H(\theta^{[r]};\theta^{[r-1]}) =\sum_{i=1}^n\sum_{k=1}^KE_{\theta^{[r-1]}}[z_{i,k}ln(t_{ik}(\theta^{[r]}))|X_i=x_i]
$$
et $E_{\theta^{[r-1]}}[.]$ l&rsquo;espérance prise en considérant la distribution mélange avec le paramère $\theta^{[r-1]}$.</p><p>Rappel : à chaque étape on cherche à maximiser un paramètre $\theta^{[r]}$ en se servant de la valeur qu&rsquo;il avait à l&rsquo;itération prédédente pour calculer la vraissamblance complétée.
dans les écritures de $H$ et $Q$, $\theta^{[r-1]}$ est le paramètre de l&rsquo;itération précédente, $\theta^{[r]}$ est le nouveau paramètre à rechercher pour maximiser la vraissemblance.</p><p>$$
l(\theta^{[r]};x)=\sum_{i=1}^n ln(p(xi;\theta^{[r]}))= \sum_{i=1}^n\sum_{k=1}^Kz_{i,k}ln(\pi_k^{[r]}p_k(x_i;\alpha_k^{[r]})) - \sum_{i=1}^n\sum_{k=1}^Kz_{i,k}ln(t_{i,k}(\theta^{[r]}))
$$
(Voir formule d&rsquo;Hathaway plus haut)</p><p>On passe à l&rsquo;espérance sous le paramètre $\theta^{[r-1]}$ conditionnellement à $X_i=x_i$
La seule chose aléatoire, ce sont les $z_{i,k}$, qui vont dont être estimés par leur espérance qui se calcule dans le cadre d&rsquo;un paramétrage, ici celui de l&rsquo;itération précédente $\theta^{[r-1]}$
$$
\sum_{i=1}^n E_{\theta^{[r-1]}}[ln(p(xi;\theta^{[r]}))|X_i=x_i]= \sum_{i=1}^n\sum_{k=1}^KE_{\theta^{[r-1]}}[z_{i,k}ln(\pi_k^{[r]}p_k(x_i;\alpha_k^{[r]}))|X_i=x_i] - \sum_{i=1}^n\sum_{k=1}^KE_{\theta^{[r-1]}}[z_{i,k}ln(t_{i,k}(\theta^{[r]}))|X_i=x_i]
$$
le premier terme est déterministe, il s&rsquo;agit de l&rsquo;espérance d&rsquo;un scalaire
$$
\iff \sum_{i=1}^n ln(p(xi;\theta^{[r]})) = \sum_{i=1}^n\sum_{k=1}^KE_{\theta^{[r-1]}}[z_{i,k}ln(\pi_k^{[r]}p_k(x_i;\alpha_k^{[r]}))|X_i=x_i] - \sum_{i=1}^n\sum_{k=1}^KE_{\theta^{[r-1]}}[z_{i,k}ln(t_{i,k}(\theta^{[r]}))|X_i=x_i]
$$
on resimplifie ensuite les expressions de log vraissemblance et log vraissemblance complétée pour revenir à la formule
$$
\iff l(\theta^{[r]};x) = E_{\theta^{[r-1]}}[l(\theta^{[r]};x,z)|X_i=x_i] - \sum_{i=1}^n\sum_{k=1}^KE_{\theta^{[r-1]}}[z_{i,k}ln(t_{i,k}(\theta^{[r]}))|X_i=x_i]
$$
$$
\iff l(\theta^{[r]};x) = Q(\theta^{[r]},\theta^{[r-1]}) - H(\theta^{[r]},\theta^{[r-1]})
$$</p><p><em>Écrire la différence entre les log-vraisemblances obtenues aux itérations $r$ et $r-1$ à partir des fonctions Q et H</em></p><p>$$
l(\theta^{[r]};x)-l(\theta^{[r-1]};x)=Q(\theta^{[r]},\theta^{[r-1]}) - Q(\theta^{[r-1]},\theta^{[r-1]}) - H(\theta^{[r]},\theta^{[r-1]}) + H(\theta^{[r-1]},\theta^{[r-1]})
$$</p><p>On s&rsquo;intéresse donc à l&rsquo;évolution de la vraissemblance entre l&rsquo;ancienne valeur (itération précédente) $\theta^{[r-1]}$ et la nouvelle valeur $\theta^{[r]}$, pour cela on le décompose sous le paramétrage de l&rsquo;itération précédente fixé $\theta^{[r-1]}$</p><p><em>Montrer que :</em> $Q(\theta^{[r]};\theta^{[r-1]})- Q(\theta^{[r-1]};\theta^{[r-1]})\geq0$</p><p>$$
Q(\theta^{[r]};\theta^{[r-1]}) = \max_{\theta \in \Theta} Q(\theta;\theta^{[r-1]})
$$
en effet, par construction de l&rsquo;algorithme EM
$$
\theta^{[r]} = \arg\max_{\theta \in \Theta} \sum_{i=1}^{n}\sum_{k=1}^{K}t_{i,k}(\theta^{[r-1]})ln(\pi_kp_k(x_i;\alpha_k)) = \arg\max_{\theta \in \Theta} E_{\theta^{[r-1]}}[l(\theta;x,z)|X_i=x_i]
$$
et $Q(\theta;\theta^{[r-1]}) =E_{\theta^{[r-1]}}[l(\theta;x,z)|X_i=x_i]$
La quantité $Q$ n&rsquo;est rien d&rsquo;autre que la vraissemblance complété calculée avec $\theta^{[r-1]}$
$$
\implies Q(\theta^{[r]};\theta^{[r-1]}) ≥ Q(\theta^{[r-1]};\theta^{[r-1]})
$$</p><p><em>En utilisant l’<a href=https://fr.wikipedia.org/wiki/In%C3%A9galit%C3%A9_de_Jensen>inégalité de Jensen</a>, montrer que :</em> $H(\theta^{[r]};\theta^{[r-1]})-H(\theta^{[r-1]};\theta^{[r-1]})≤0$</p><p>$$
H(\theta^{[r]};\theta^{[r-1]})-H(\theta^{[r-1]};\theta^{[r-1]})
$$
$$
=\sum_{i=1}^n\sum_{k=1}^nE_{\theta^{[r-1]}}[z_{i,k}ln(t_{ik}(\theta^{[r]}))|X_i=x_i] - \sum_{i=1}^n\sum_{k=1}^nE_{\theta^{[r-1]}}[z_{i,k}ln(t_{ik}(\theta^{[r-1]}))|X_i=x_i]
$$</p><p>Attention : $ln(t_{ik}(\theta^{quelconque}))$ est déterministe ! La seule chose aléatoire dans les espérance ce sont les $z_{i,k}$</p><p>$$
=\sum_{i=1}^n\sum_{k=1}^n ln(t_{ik}(\theta^{[r]})) E_{\theta^{[r-1]}}[z_{i,k}|X_i=x_i] - \sum_{i=1}^n\sum_{k=1}^n ln(t_{ik}(\theta^{[r-1]})) E_{\theta^{[r-1]}}[z_{i,k}|X_i=x_i]
$$
et $E_{\theta^{[r-1]}}[z_{i,k}|X_i=x_i]$ est la définition de $t_{i,k}(\theta^{[r-1]})$
$$
=\sum_{i=1}^n\sum_{k=1}^n t_{ik}(\theta^{[r-1]})[ln(t_{ik}(\theta^{[r]})) - ln(t_{ik}(\theta^{[r-1]}))]
$$
$$
=\sum_{i=1}^n\sum_{k=1}^n t_{ik}(\theta^{[r-1]})ln(\frac{t_{ik}(\theta^{[r]})}{t_{ik}(\theta^{[r-1]})})
$$</p><p>On utilise Jensen et la concavité du logarithme ainsi :
$$
\sum_{k=1}^Ku_kln(v_k) \leq ln(\sum_{k=1}^Ku_kv_k)
$$
si les $u_k$ positifs et $\sum u_k=1$</p><p>$t_{ik}(\theta^{[r-1]}) \geq 0$ et $\sum_k t_{ik}(\theta^{[r-1]})=1$ car i appartient forcément à une des classes, donc la somme des probabilités d&rsquo;appartenance à chacune des classes vaut 1</p><p>On remplace donc $u_k$ par $t_{ik}(\theta^{[r-1]})$ et $v_k$ par $\frac{t_{ik}(\theta^{[r]})}{t_{ik}(\theta^{[r-1]})}$ et on conclue</p><p>$$
H(\theta^{[r]};\theta^{[r-1]})-H(\theta^{[r-1]};\theta^{[r-1]})≤\sum_{i=1}^nln(\sum_{k=1}^Kt_{ik}(\theta^{[r]}))=0
$$</p><p>En cumulant les deux résultats précédents, on aboutit à
$$
l(\theta^{[r]};x)-l(\theta^{[r-1]};x)\geq 0 \iff l(\theta^{[r]};x) \geq l(\theta^{[r-1]};x)
$$</p><p>On obtient qu&rsquo;à chaque itération la vraissemblance augmente forcément, ce qui assure que l&rsquo;algorithme va forcément converger vers un maximum local. (Au pire on stagne, ce qui sgnifie qu&rsquo;on a atteint ce maximum)</p><h1 id=exercice-2--mélanges-de-distribution-de-poisson>Exercice 2 : Mélanges de distribution de Poisson <a href=#exercice-2--m%c3%a9langes-de-distribution-de-poisson class=anchor aria-hidden=true>#</a></h1><p>La <a href=https://fr.wikipedia.org/wiki/Loi_de_Poisson>loi de Poisson</a> est une loi discrète définie par $p(X=x)=\frac{\lambda^x}{x!}e^{-\lambda}$
Elle posède donc un paramètre : $\lambda$</p><p>On considère un échantillon $x= (x_1,&mldr;,x_n)$ composé de $n$ observations indépendantes $x_i∈\mathbb{N}$. Pour effectuer le clustering, on suppose que les données sont issues d’un mélange de distributions de Poisson à $K$ composantes.</p><p>Si on mélange $K$ lois de poisson, on a donc $2K$ paramètres à estimer, $K$ $\lambda_k$ et $K$ $\pi_k$ (donc un forcé par contrainte $\sum \pi_k =1$)
$\theta = (\lambda_1,&mldr;,\lambda_K,\pi_1,&mldr;,\pi_K)$</p><p>Écriture générale
$$
p(x_i;\theta) = \sum_{k=1}^K \pi_k p_k(x_i;\lambda_k)
$$
ici :
$$
p(x_i;\theta) = \sum_{k=1}^K \pi_k \frac{{\lambda_k}^{x_i}}{x_i!}e^{-\lambda_k}
$$
Les $\lambda_k$ sont positifs, et différents 2 à 2 (si 2 sont identiques, c&rsquo;est la même composante)</p><p>Dans ces conditions, le modèle est-il identifiable ?
C&rsquo;est à dire :
si $\forall x \in \mathbb{N}, p(x,\theta)=p(x,\tilde \theta) \implies \theta = \tilde \theta$
Dans le cas d&rsquo;un mélange l&rsquo;unicité se définit &ldquo;à permutation des composantes près&rdquo;
Pour cela on choisit par exemple de trier les $\lambda_k$ du plus grand au plus petit : contrainte sur &ldquo;l&rsquo;ordre&rdquo; des composantes</p><p>Supposons $\forall x \in \mathbb{N}, p(x,\theta)=p(x,\tilde \theta)$
On choisit que si $\lambda_1 \neq \tilde \lambda_1$ alors on &ldquo;classe&rdquo; $\theta$ et $\tilde \theta$ tel que $\lambda_1 > \tilde \lambda_1$, c&rsquo;est à dire qu&rsquo;on suppose $\lambda_1 \geq \tilde \lambda_1$</p><p>$$
p(x,\theta)=p(x,\tilde \theta)
$$
$$
\iff \sum_{k=1}^K \pi_k \frac{{\lambda_k}^{x_i}}{x_i!}e^{-\lambda_k}=\sum_{k=1}^K \tilde \pi_k \frac{{\tilde \lambda_k}^{x_i}}{x_i!}e^{-\tilde \lambda_k}
$$
$$
\iff \pi_1 \frac{{\lambda_1}^{x_i}}{x_i!}e^{-\lambda_1} + \sum_{k=2}^K \pi_k \frac{{\lambda_k}^{x_i}}{x_i!}e^{-\lambda_k}=\tilde \pi_1 \frac{{\tilde \lambda_1}^{x_i}} {x_i!}e^{-\tilde \lambda_1} + \sum_{k=2}^K \tilde \pi_k \frac{{\tilde \lambda_k}^{x_i}}{x_i!}e^{-\tilde \lambda_k}
$$
$$
\iff 1 + \sum_{k=2}^K \frac{\pi_k}{\pi_1} \frac{{\lambda_k}^{x_i}}{{\lambda_1}^{x_i}}e^{\lambda_1-\lambda_k}=\frac{\tilde \pi_1}{\pi_1} \frac{{\tilde \lambda_1}^{x_i}}{{\lambda_1}^{x_i}}e^{\lambda_1 - \tilde \lambda_1} + \sum_{k=2}^K \frac{\tilde \pi_k}{\pi_1} \frac{{\tilde \lambda_k}^{x_i}}{{\lambda_1}^{x_i}}e^{\lambda_1-\tilde \lambda_k}
$$
Par hypothèse, on a $\lambda_1>\lambda_k$ et $\lambda_1 \geq \tilde \lambda_1 > \tilde \lambda_k$
Donc :
$\forall k>1,\frac{\lambda_k}{\lambda_1}&lt;1$
$\forall k>1,\frac{\tilde \lambda_k}{\lambda_1}&lt;1$
Donc :
$\forall k>1,\lim_{x\to\infty}\frac{\pi_k}{\pi_1}(\frac{\lambda_k}{\lambda_1})^{x_i}e^{(\lambda_1-\lambda_k)}=0$
$\forall k>1,\lim_{x\to\infty}\frac{\pi_k}{\pi_1}(\frac{\tilde \lambda_k}{\lambda_1})^{x_i}e^{(\lambda_1-\tilde \lambda_k)}=0$</p><p>Soit en revenant à l&rsquo;égalité précédente
$$
\iff 1 = \lim_{x\to\infty}\frac{\tilde \pi_1}{\pi_1} \frac{{\tilde \lambda_1}^{x_i}}{{\lambda_1}^{x_i}}e^{\lambda_1 - \tilde \lambda_1}
$$
et
$$
\lim_{x\to\infty} \frac{\tilde \pi_1}{\pi_1} \frac{{\tilde \lambda_1}^{x_i}}{{\lambda_1}^{x_i}}e^{\lambda_1 - \tilde \lambda_1} = \frac{\tilde \pi_1}{\pi_1} \text{ si } \lambda_1 = \tilde \lambda_1
$$
$$
\lim_{x\to\infty} \frac{\tilde \pi_1}{\pi_1} \frac{{\tilde \lambda_1}^{x_i}}{{\lambda_1}^{x_i}}e^{\lambda_1 - \tilde \lambda_1} = 0\text{ si } \lambda_1 > \tilde \lambda_1
$$</p><p>Donc (par l&rsquo;absurde) $\lambda_1 = \tilde \lambda_1$ et de fait $\pi_1 = \tilde \pi_1$</p><p>Une fois qu&rsquo;on a montré le résultat pour ce premier paramètre on effectue le même raisonnement pour les paramètres suivant</p><h2 id=estimation-par-maximum-de-vraissemblance>Estimation par maximum de vraissemblance <a href=#estimation-par-maximum-de-vraissemblance class=anchor aria-hidden=true>#</a></h2><p>$$
l(\theta;x)=\sum_{i=1}^nln(\sum_{k=1}^n \pi_k \frac{\lambda_k^{x_i}}{x_i!}e^{-\lambda_k})
$$
$$
l(\theta;x,z)=\sum_{i=1}^n\sum_{k=1}^nz_{i,k}ln( \pi_k \frac{\lambda_k^{x_i}}{x_i!}e^{-\lambda_k})
$$
$$
=\sum_{i=1}^n\sum_{k=1}^nz_{i,k}(ln( \pi_k ) + x_iln(\lambda_k) - ln(x_i!) -\lambda_k)
$$</p><p><strong>Algorithme EM</strong></p><p>On choisit au hasard les $\pi_k^{[0]}$, tels que $0&lt;\pi_k^{[0]}&lt;1$ et $\sum \pi_k^{[0]} =1$
ainsi que les $\lambda_k^{[0]}$ positifs</p><p>On choisit un $\epsilon$ petit qui sera le seuil à partir du quel on cosidèrera que l&rsquo;évolution est minime, et que l&rsquo;on arrive ainsi au maximum</p><p>Soit un critère d&rsquo;arrêt de l&rsquo;algorithme
$l(\theta^{[r]};x) - l(\theta^{[r-1]};x)\leq\epsilon$</p><p>A chaque étape</p><ul><li><p>On détermine une estimation des $z_{ik}$, les $t_{ik}$ calculés à l&rsquo;aide des apriori, cad les paramètres de l&rsquo;itération précédente :
$$
t_{ik}(\theta^{[r-1]})=\frac{\pi_k^{[r-1]}p_k(x_i;\lambda_k^{[r-1]})}{\sum_l \pi_l^{[r-1]}p_l(x_i;\lambda_l^{[r-1]})}
$$</p></li><li><p>On maximise la vraissemblance complétée que l&rsquo;on sait désormais calculer
$l(\theta;x,z)$ non calculable car dépendant des $z_{ik}$ inconnus devient $l(\theta;x,t(\theta^{[r-1]}))$
avec $t(\theta^{[r-1]})$ l&rsquo;ensemble des $t_{ik}(\theta^{[r-1]})$
Soit la recheche de
$$
\theta^{[r]}=\arg\max_{\theta}l(\theta;x,t(\theta^{[r-1]}))=\arg\max_{\theta}\sum_{i=1}^n\sum_{k=1}^nt_{ik}(\theta^{[r-1]})ln( \pi_k \frac{\lambda_k^{x_i}}{x_i!}e^{-\lambda_k})
$$</p></li></ul><p><em>Calcul des $\lambda_k$</em></p><p>$$
\frac{\partial}{\partial \lambda_k}l(\theta;x,t(\theta^{[r-1]}))= \sum_i t_{ik}(\theta^{[r-1]})\frac{\partial}{\partial \lambda_k}(ln(\pi_k) -ln(x_i!)+x_iln(\lambda_k)-\lambda_k)
$$
$$
= \sum_i t_{ik}(\theta^{[r-1]})(\frac{x_i}{\lambda_k}-1)
$$
⚠️ les $t_{ik}(\theta^{[r-1]})$ ne dépendent pas de $\lambda_k$ (mais de $\lambda_k^{[r-1]})$ !
$$
\frac{\partial}{\partial \lambda_k}l(\theta;x,t(\theta^{[r-1]}))= 0 \iff \frac{1}{\lambda_k^{[r]}}\sum_i t_{ik}(\theta^{[r-1]})x_i = \sum_i t_{ik}(\theta^{[r-1]})
$$
$$
\iff \lambda_k^{[r]} = \frac{\sum_i t_{ik}(\theta^{[r-1]})x_i}{\sum_i t_{ik}(\theta^{[r-1]})}
$$</p><p><em>Calcul des $\pi_k$</em></p><p>Il s&rsquo;agit d&rsquo;une estimation sous contrainte ($\sum_k \pi_k =1$), on utilise donc un lagrangien</p><p>$L(\theta,\mu)=l(\theta;x, t_{ik}(\theta^{[r-1]})) - \mu (\sum_k \pi_k - 1)$
$$
\frac{\partial}{\partial \pi_k} L(\theta) = \sum_i t_{ik}(\theta^{[r-1]}) \frac{\partial}{\partial \pi_k}(ln(\pi_k) -ln(x_i!)+x_iln(\lambda_k)-\lambda_k) - \mu
$$
$$
= \sum_i \frac{t_{ik}(\theta^{[r-1]}) }{\pi_k} - \mu
$$
(dériver selon $\mu$ fait simplement retrouver la contrainte $\sum_k\pi_k=1$)
$$
\frac{\partial}{\partial \pi_k} L(\theta) = 0 \iff \pi_k^{[r]}=\frac{1}{\mu}\sum_i t_{ik}(\theta^{[r-1]})
$$
puis on a par contrainte
$$
\sum_k \pi_k^{[r]}=1=\frac{1}{\mu}\sum_k\sum_i t_{ik}(\theta^{[r-1]})
$$
par construction
$$
\sum_k t_{ik}(\theta^{[r-1]}) = 1
$$
donc
$$
\sum_k\sum_i t_{ik}(\theta^{[r-1]}) =n
$$
soit $\mu=n$ et
$$
\pi_k^{[r]}=\frac{1}{n}\sum_i t_{ik}(\theta^{[r-1]})
$$
Au final étant donné que les $t_{ik}$ sont les espérances d&rsquo;appartenance à chacune des classes, la somme est donc l&rsquo;espérance de l&rsquo;effectif de chacune des classes et on ne fait donc que calculer une fréquence (espérance du nombre d&rsquo;individu dans la classe divisé par n) !</p><p><em>Itération suivante</em></p><p>Connaissant les nouveaux paramètres de $\theta$ on peut calculer la nouvelle valeur de la vraissemblance $l(\theta^{[r]};x)$ que l&rsquo;on compare à celle de l&rsquo;itération précédente</p><h3 id=remarque--algorithme-cem>Remarque : algorithme CEM <a href=#remarque--algorithme-cem class=anchor aria-hidden=true>#</a></h3><p>C&rsquo;est l&rsquo;algorithme EM plus réaliste du point de vue de la classification.
Au lieu de remplacer les $z_{ik}$ par leur espérance $t_{ik}$, on effectue la classification, c&rsquo;est à dire que l&rsquo;on affecte chaque $i$ dans la classe où il est le plus probable, cad vérifiant $\max_kt_{ik}$
On remplace donc les $z_{ik}$ par des valeurs plus réelles, 1 si $i$ a été affecté à $k$ lors de l&rsquo;itération $[r-1]$, 0 sinon.
Dans l&rsquo;algorithme CEM, les $\pi_k^{[r]}$ sont les vrais proportions des classes formées à l&rsquo;itération $[r-1]$</p><h3 id=remarque--méthode-aléatoire>Remarque : méthode aléatoire <a href=#remarque--m%c3%a9thode-al%c3%a9atoire class=anchor aria-hidden=true>#</a></h3><p>Comme pour les k-means, la méthode peut aboutir à un maximum local. Il faut donc relancer plusieurs fois l&rsquo;algorithme, et conserver le cas qui a donné le plus haut résultat de vraissemblance (le maximum des maximum locaux a des chances d&rsquo;être le maximum global)</p><h3 id=remarque--choisir-le-nombre-de-classes>Remarque : choisir le nombre de classes <a href=#remarque--choisir-le-nombre-de-classes class=anchor aria-hidden=true>#</a></h3><p>On pourrait se dire que l&rsquo;on choisit $K$ tel que le maximum de vraissemblance obtenu soit le meilleur possible.
Mais comme avec l&rsquo;inertie intra des méthodes géométriques, mécaniquement en augmentant K, le maximum de vraissemblance sera toujours plus élevé.
En effet $K$ plus élevé signifie un plus grand nombre de paramètres, ce qui améliore forcément le modèle.
On utilise au final des critères qui pénalisent le nombre de paramètre, AIC et BIC. C&rsquo;est le même raisonnement que lorsqu&rsquo;on observe un coude dans les gains d&rsquo;inertie intra, on se demande si l&rsquo;ajout de paramètre est &ldquo;rentable&rdquo;.</p><h1 id=exercice-3--lien-entre-clustering-géométrique-et-probabiliste>Exercice 3 : Lien entre clustering géométrique et probabiliste <a href=#exercice-3--lien-entre-clustering-g%c3%a9om%c3%a9trique-et-probabiliste class=anchor aria-hidden=true>#</a></h1><p>On considère un échantillon $x=(x1,&mldr;,xn)$ composé de $n$ observations indépendantes. Chaque observation $x_i∈\mathbb{R}^d$ est décrite par $d$ variables continues.
Montrer que le clustering de ces données en $K$ classes effectué par un algorithme des Kmeans avec la métrique diagonale $M=diag(\frac{1}{s^2_1},&mldr;,\frac{1}{s^2_d})$ (<strong>métrique inverse des variances</strong>) est équivalent à un clustering effectué par un algorithme CEM considérant le modèle défini par la densité suivante :</p><p>$$
p(x_i;\theta) =\sum_{k=1}^{K}\frac{1}{K}\prod_{j=1}^{d}\phi(x_{ij};\mu_{kj},s^2_j)
$$
avec
$$
\phi(x_{ij};\mu_{kj},s^2_j)=\frac{1}{s_j\sqrt{2\pi}}e^{-\frac{1}{2}(\frac{x_{ij}-\mu_{kj}}{s_j})^2}
$$
la densité d&rsquo;une loi normale d&rsquo;espérance $\mu_{kj}$ et de variance $s^2_j$</p><ul><li>Vision géométrique : voir le premier exercice du TP ou l&rsquo;on a créé 4 groupes de points avec des moyennes différentes</li><li>Vision probabiliste : on observe une ditribution avec $K$ &ldquo;cloches&rdquo; superposées</li></ul><p><strong>Clustering Kmeans</strong>
Détermination de l&rsquo;estimateur $(\tilde z,\tilde \mu)$, où les $\mu_k$ sont les centres obtenus et les $z_{ik}$ calculé selon les points dont $\mu_k$ est le plus proche
$$
(\tilde z,\tilde \mu) = \arg\min_{z,\mu} \sum_i\sum_k z_{ik} d^2_M(x_i,\mu_k)
$$
En effet on recherche la configuration selon laquelle l&rsquo;inertie intra est la plus faible, càd telles que la somme de la somme des distances des points à leur baycentre est la plus faible
$$
= \arg\min_{z,\mu} \sum_i\sum_k z_{ik} \sum_{j=1}^d\frac{(x_{ij} -\mu_{kj})^2}{s^2_j}
$$
le barycentre de classe k étant le point de coordonnées $\mu_k=(\mu_{k,1},&mldr;,\mu_{k,d})$</p><p><strong>Clustering CEM</strong>
Détermination de l&rsquo;estimateur $(\hat z,\hat \mu)$ obtenu par maximum de vraissemblance
$$
(\hat z,\hat \mu)=\arg\max_{z,\mu} \sum_i\sum_k z_{i_k}ln(\frac{1}{K}\prod_{j=1}^d\phi(x_{ij};\mu_{kj},s^2_j))
$$
$$
=\arg\max_{z,\mu} \sum_i\sum_k z_{i_k}ln(\frac{1}{K}) +z_{i_k} \sum_{j=1}^dln(\phi(x_{ij};\mu_{kj},s^2_j))
$$
$$
=\arg\max_{z,\mu} \sum_i\sum_k [ z_{i_k}ln(\frac{1}{K}) - z_{i_k} \frac{1}{2}\sum_{j=1}^dln(2\pi s_j^2) -z_{i_k} \frac{1}{2} \sum_{j=1}^d\frac{(x_{ij}-\mu_{kj})^2}{s^2_j} ]
$$
$$
=\arg\max_{z,\mu} nln(\frac{1}{K}) - n\frac{1}{2}\sum_{j=1}^dln(2\pi s_j^2) - \sum_i\sum_kz_{i_k} \frac{1}{2} \sum_{j=1}^d\frac{(x_{ij}-\mu_{kj})^2}{s^2_j}
$$
$$
=\arg\max_{z,\mu} - \sum_i\sum_kz_{i_k} \sum_{j=1}^d\frac{(x_{ij}-\mu_{kj})^2}{s^2_j}
$$
$$
=\arg\min_{z,\mu} \sum_i\sum_kz_{i_k} \sum_{j=1}^d\frac{(x_{ij}-\mu_{kj})^2}{s^2_j}
$$</p><div class="page-footer-meta d-flex flex-column flex-md-row justify-content-between"></div><div class="docs-navigation d-flex justify-content-between"><a href=/docs/ensai/classification/td1/><div class="card my-1"><div class="card-body py-2">&larr; TD1 Classification</div></div></a><a class=ms-auto href=/docs/ensai/classification/tp/><div class="card my-1"><div class="card-body py-2">TP Classification &rarr;</div></div></a></div></main></div></div></div><footer class="footer text-muted"><div class=container-xxl><div class=row><div class="col-lg-8 order-last order-lg-first"><ul class=list-inline><li class=list-inline-item>Powered by <a class=text-muted href=https://www.netlify.com/>Netlify</a>, <a class=text-muted href=https://gohugo.io/>Hugo</a>, and <a class=text-muted href=https://getdoks.org/>Doks</a></li></ul></div><div class="col-lg-8 order-first order-lg-last text-lg-end"><ul class=list-inline></ul></div></div></div></footer><script src=/js/bootstrap.min.f93d87ed312fd94eae9a720e5988a3bd322bcdd099518866fc5668e18610cca0077fc356b3edfc727dedf5f773dab0a6d87a870e4b616595e76381163dd5a730.js integrity="sha512-+T2H7TEv2U6umnIOWYijvTIrzdCZUYhm/FZo4YYQzKAHf8NWs+38cn3t9fdz2rCm2HqHDkthZZXnY4EWPdWnMA==" crossorigin=anonymous defer></script>
<script src=/js/highlight.min.63c18be47c0d2739f962f289a2e583af3c2406e1cb7e2da757e1bce99a3ea515457adce40c34e39048dc180e5f16bb4346dc6e5f627ad27beb158d619e050779.js integrity="sha512-Y8GL5HwNJzn5YvKJouWDrzwkBuHLfi2nV+G86Zo+pRVFetzkDDTjkEjcGA5fFrtDRtxuX2J60nvrFY1hngUHeQ==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/katex.min.93253b59f879e8b8bf4ef8eec49737249299a1cc8b9c25eb1b0c94ae722f3bc470677d56dfdfcc838ccc904ee3ebcc46a5336495f3b1d7fdbb182f3e6b952390.js integrity="sha512-kyU7Wfh56Li/TvjuxJc3JJKZocyLnCXrGwyUrnIvO8RwZ31W39/Mg4zMkE7j68xGpTNklfOx1/27GC8+a5UjkA==" crossorigin=anonymous defer></script>
<script src=/js/vendor/katex/dist/contrib/auto-render.min.b688a23169501c711300d4bdb5ae40c52ee8976f722fbf3871eb709a51db2ea5d7bf51eaef0e82af7542d973d2f681d9c46b70b72d902601642a15af9b9ce80d.js integrity="sha512-toiiMWlQHHETANS9ta5AxS7ol29yL784cetwmlHbLqXXv1Hq7w6Cr3VC2XPS9oHZxGtwty2QJgFkKhWvm5zoDQ==" crossorigin=anonymous defer></script>
<script src=/main.min.b8e7e565bd2186f67f57d2c5d9680f67abbb2a977ff20ad27eb51f5a7b9071fdec7b71a5e964a41a38fb59cb12d43b76e0b3ac8247cb7063bb73efc06a9ac529.js integrity="sha512-uOflZb0hhvZ/V9LF2WgPZ6u7Kpd/8grSfrUfWnuQcf3se3Gl6WSkGjj7WcsS1Dt24LOsgkfLcGO7c+/AaprFKQ==" crossorigin=anonymous defer></script>
<script src=https://formation.dufau.re/index.min.2c640cac86912b040cf7553f94d954c4cc9e6f25517bb9b02a993b472f2b06b5534cbad3f1004c86b8dacdbf38a43719936bf3c06f64de03a3d748b1fb144901.js integrity="sha512-LGQMrIaRKwQM91U/lNlUxMyebyVRe7mwKpk7Ry8rBrVTTLrT8QBMhrjazb84pDcZk2vzwG9k3gOj10ix+xRJAQ==" crossorigin=anonymous defer></script></body></html>