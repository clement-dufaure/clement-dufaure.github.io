<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Classification on</title><link>https://formation.dufau.re/docs/ensai/classification/</link><description>Recent content in Classification on</description><generator>Hugo -- gohugo.io</generator><language>fr-FR</language><atom:link href="https://formation.dufau.re/docs/ensai/classification/index.xml" rel="self" type="application/rss+xml"/><item><title>TD1 Classification</title><link>https://formation.dufau.re/docs/ensai/classification/td1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://formation.dufau.re/docs/ensai/classification/td1/</guid><description>Exercice 1 : K Means # 1-1 Différents paramétrages # | 1 | 2 | 9 | 12 | 20 |
a) $K=2;μ_1=1;μ_2=7$
K nombre de classe
Dans K-means, le nombre de classe est un paramètre, il est défini a priori
On choisit alors au hasard autant de points que de classes (notés $µ_i$), ce sont les barycentres des classes à l&amp;rsquo;itération 0.
Itération 1 # 1 2 9 12 20 x x $µ_1=1$ $µ_2=7$ Points plus proche de $µ_1$ que de $µ_2$ : 1,2</description></item><item><title>TD2 Classification</title><link>https://formation.dufau.re/docs/ensai/classification/td2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://formation.dufau.re/docs/ensai/classification/td2/</guid><description>Exercice 1 : Mélanges et algorithme EM # Préambule # Qu&amp;rsquo;est ce qu&amp;rsquo;un &amp;ldquo;mélange&amp;rdquo; de lois ? On considère un échantillon $x= (x1,&amp;hellip;,xn)$ composé de $n$ observations indépendantes $x_i∈X$. Chaque observation est issue d’un mélange à $K$ composantes déterminées par $p(x_i;\theta)=\sum_{k=1}^{K}\pi_kp_k(x_i;\alpha_k)$,avec $\theta \in \Theta$.
= loi inconnue compliquée à décrire, à paramétrer = 2 lois normales superposées Idée :
chaque point a une probabilité $\pi_k$ d&amp;rsquo;appartenir à une des deux courbes (= proportion des points correspondant à chacune des 2 courbes, bien entendue inconnue), $\sum \pi_k =1$ la loi de chaque courbe est $p_k(x_i;\alpha_k)$ avec $\alpha_k$ les paramètres de la loi $p_k$ (si $p_k$ est une loi normale $\alpha_k=(\mu_k,\sigma_k)$) Ce qui donne en additionnant chaque loi probable $p(x_i;\theta)=\sum_{k=1}^{K}\pi_kp_k(x_i;\alpha_k)$ a)</description></item><item><title>TP Classification</title><link>https://formation.dufau.re/docs/ensai/classification/tp/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>https://formation.dufau.re/docs/ensai/classification/tp/</guid><description>TP2 exo2
rm(list = ls()) install.packages(&amp;quot;mclust&amp;quot;) require(cluster) require(pgmm) require(FactoMineR) require(explor) require(VarSelLCM) require(mclust) data(&amp;quot;coffee&amp;quot;) ?coffee summary(coffee) coffee$Variety &amp;lt;- as.factor(coffee$Variety) out.pca &amp;lt;- PCA(coffee, quali.sup = 1:2, graph = F) explor(out.pca) plot(out.pca$ind$coord[,1], out.pca$ind$coord[,2], col=coffee[,1]) ?VarSelCluster res.diago.bic &amp;lt;- VarSelCluster(coffee[, -c(1, 2)], 1:8, vbleSelec = F) summary(res.diago.bic) res.diago.icl &amp;lt;- VarSelCluster(coffee[, -c(1, 2)], 1:8, vbleSelec = F, crit.varsel = &amp;quot;ICL&amp;quot;) summary(res.diago.icl) res.diago.bic@partitions table(coffee[, 1], res.diago.bic@partitions@zMAP) table(coffee[, 2], res.diago.bic@partitions@zMAP) par(mfrow=c(1,1)) plot(out.pca$ind$coord[, 1], out.pca$ind$coord[, 2], col = res.diago.bic@partitions@zMAP) sil &amp;lt;- silhouette(res.</description></item></channel></rss>